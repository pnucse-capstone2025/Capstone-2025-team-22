#!/usr/bin/env python3
"""ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ ë””ë²„ê¹…"""

import torch
import sys
import os

# ë¶€ëª¨ ë””ë ‰í† ë¦¬ ì¶”ê°€
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
grandparent_dir = os.path.dirname(parent_dir)
sys.path.insert(0, grandparent_dir)

from data import load_data, KeywordDataset, Collator
from model import KoKeyBERT
from real_distillation_gpu import extract_keywords_from_bio_tags, evaluate_keywords, create_real_models
from torch.utils.data import DataLoader

def debug_model_predictions():
    """ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ ë””ë²„ê¹…"""
    print("ğŸ” ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ ë””ë²„ê¹…")
    
    device = torch.device('cpu')
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    try:
        sys.path.append('../../kobert_tokenizer')
        from kobert_tokenizer import KoBERTTokenizer
        tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')
        print("âœ… KoBERT í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ")
    except Exception as e:
        print(f"âš ï¸ KoBERT í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {e}")
        from transformers import BertTokenizer
        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
        print("âœ… ê¸°ë³¸ BERT í† í¬ë‚˜ì´ì €ë¡œ ëŒ€ì²´")
    
    # ë°ì´í„° ë¡œë“œ
    print("\nğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”© ì¤‘...")
    test_data_path = "../../../src/data/test_clean.json"
    test_data = load_data(test_data_path)
    
    print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_data)} í•­ëª©")
    
    # Datasetê³¼ DataLoader ìƒì„±
    test_dataset = KeywordDataset(test_data)
    collator = Collator(tokenizer)
    test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collator)
    
    # ëª¨ë¸ ìƒì„±
    print("\nğŸ—ï¸ ëª¨ë¸ ìƒì„± ì¤‘...")
    teacher_model, student_model = create_real_models(device)
    teacher_model.eval()
    student_model.eval()
    
    # ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
    print("\n" + "="*50)
    print("ğŸ“‹ ëª¨ë¸ ì˜ˆì¸¡ ë¶„ì„")
    print("="*50)
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(test_loader):
            if batch_idx >= 1:  # ì²« ë²ˆì§¸ ë°°ì¹˜ë§Œ
                break
                
            # test.py ë°©ì‹ì˜ batch í˜•ì‹: (index, input_ids, attention_mask, tags)
            index, input_ids, attention_mask, tags = batch
            input_ids = input_ids.to(device)
            attention_mask = attention_mask.to(device)
            tags = tags.to(device)
            
            print(f"ë°°ì¹˜ í¬ê¸°: {input_ids.size(0)}")
            print(f"ì‹œí€€ìŠ¤ ê¸¸ì´: {input_ids.size(1)}")
            
            # Teacher ëª¨ë¸ ì˜ˆì¸¡
            print("\nğŸ« Teacher ëª¨ë¸ ì˜ˆì¸¡:")
            teacher_output = teacher_model(input_ids, attention_mask, tags)
            if isinstance(teacher_output, tuple):
                teacher_log_likelihood, teacher_sequence = teacher_output
                print(f"Teacher ì¶œë ¥ í˜•íƒœ: (log_likelihood, sequence)")
                print(f"log_likelihood shape: {teacher_log_likelihood.shape if hasattr(teacher_log_likelihood, 'shape') else 'scalar'}")
                print(f"sequence length: {len(teacher_sequence)}")
                
                # Teacher ì˜ˆì¸¡ì„ í…ì„œë¡œ ë³€í™˜
                teacher_predictions = torch.zeros_like(tags)
                for i, seq in enumerate(teacher_sequence):
                    if i < teacher_predictions.size(0) and len(seq) <= teacher_predictions.size(1):
                        teacher_predictions[i, :len(seq)] = torch.tensor(seq, dtype=torch.long, device=device)
            else:
                teacher_predictions = teacher_output
                print(f"Teacher ì¶œë ¥ í˜•íƒœ: direct predictions")
            
            # Student ëª¨ë¸ ì˜ˆì¸¡
            print("\nğŸ“ Student ëª¨ë¸ ì˜ˆì¸¡:")
            student_output = student_model(input_ids, attention_mask, tags, return_outputs=True)
            if len(student_output) == 3:
                student_loss, student_predictions, student_bert_outputs = student_output
                print(f"Student ì¶œë ¥ í˜•íƒœ: (loss, predictions, bert_outputs)")
                print(f"Student loss: {student_loss.item():.4f}")
                print(f"Student predictions type: {type(student_predictions)}")
                
                # Student ì˜ˆì¸¡ì„ í…ì„œë¡œ ë³€í™˜
                if isinstance(student_predictions, list):
                    student_pred_tensor = torch.zeros_like(tags)
                    for i, seq in enumerate(student_predictions):
                        if i < student_pred_tensor.size(0) and len(seq) <= student_pred_tensor.size(1):
                            student_pred_tensor[i, :len(seq)] = torch.tensor(seq, dtype=torch.long, device=device)
                    student_predictions = student_pred_tensor
            
            # ê° ìƒ˜í”Œë³„ ì˜ˆì¸¡ ê²°ê³¼ ë¶„ì„
            for i in range(min(2, input_ids.size(0))):
                print(f"\n--- ìƒ˜í”Œ {i+1} ---")
                
                # ì›ë³¸ ë°ì´í„°
                original_item = test_data[index[i].item()]
                actual_keywords = original_item['keyword']
                if isinstance(actual_keywords, str):
                    actual_keywords = [actual_keywords]
                
                print(f"ì‹¤ì œ í‚¤ì›Œë“œ: {actual_keywords}")
                
                # ì •ë‹µ BIO íƒœê·¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
                true_keywords = extract_keywords_from_bio_tags(
                    input_ids[i], tags[i], attention_mask[i], tokenizer
                )
                print(f"ì •ë‹µ BIO â†’ í‚¤ì›Œë“œ: {true_keywords}")
                
                # Teacher ì˜ˆì¸¡ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
                teacher_keywords = extract_keywords_from_bio_tags(
                    input_ids[i], teacher_predictions[i], attention_mask[i], tokenizer
                )
                print(f"Teacher ì˜ˆì¸¡ â†’ í‚¤ì›Œë“œ: {teacher_keywords}")
                
                # Student ì˜ˆì¸¡ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
                student_keywords = extract_keywords_from_bio_tags(
                    input_ids[i], student_predictions[i], attention_mask[i], tokenizer
                )
                print(f"Student ì˜ˆì¸¡ â†’ í‚¤ì›Œë“œ: {student_keywords}")
                
                # BIO íƒœê·¸ ë¹„êµ
                print(f"ì •ë‹µ BIO: {tags[i][:10].tolist()}...")
                print(f"Teacher BIO: {teacher_predictions[i][:10].tolist()}...")
                print(f"Student BIO: {student_predictions[i][:10].tolist()}...")
                
                # ì„±ëŠ¥ ê³„ì‚°
                teacher_tp, teacher_fp, teacher_fn = evaluate_keywords(teacher_keywords, actual_keywords)
                student_tp, student_fp, student_fn = evaluate_keywords(student_keywords, actual_keywords)
                
                print(f"Teacher: TP={teacher_tp}, FP={teacher_fp}, FN={teacher_fn}")
                print(f"Student: TP={student_tp}, FP={student_fp}, FN={student_fn}")
            
            break

if __name__ == "__main__":
    debug_model_predictions()
