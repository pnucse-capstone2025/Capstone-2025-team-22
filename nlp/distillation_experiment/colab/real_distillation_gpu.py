"""
ì‹¤ì œ KoKeyBERTì™€ ì‹¤ì œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ GPU ìµœì í™” Knowledge Distillation
ê°€ì§œ ëª¨ë¸ì´ë‚˜ ê°€ì§œ ë°ì´í„° ì—†ì´ ì§„ì§œ ì‹¤í—˜
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
import json
import os
import sys
import random
import time
from datetime import datetime
import matplotlib.pyplot as plt
import numpy as np
from transformers import BertTokenizer, BertConfig
from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score
from torch.nn.utils.rnn import pad_sequence
from typing import List

# test.py ë°©ì‹ ì‚¬ìš©ìœ¼ë¡œ ì „ì—­ collate_fn ì œê±°

# ë¶€ëª¨ ë””ë ‰í† ë¦¬ ì¶”ê°€ (ì‹¤ì œ ëª¨ë¸ importë¥¼ ìœ„í•´)
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)  # distillation_experiment
grandparent_dir = os.path.dirname(parent_dir)  # nlp
root_dir = os.path.dirname(grandparent_dir)  # 2025_CSE_graduation_assignment

sys.path.insert(0, parent_dir)
sys.path.insert(0, grandparent_dir)
sys.path.insert(0, root_dir)

def extract_keywords_from_bio_tags(tokens, bio_tags, attention_mask, tokenizer) -> List[str]:
    """
    BIO íƒœê·¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. (test.pyì™€ ë™ì¼í•œ ë¡œì§)
    
    Args:
        tokens: í† í° ID í…ì„œ
        bio_tags: BIO íƒœê·¸ í…ì„œ (0: B, 1: I, 2: O)
        attention_mask: íŒ¨ë”© ë§ˆìŠ¤í¬ í…ì„œ
        tokenizer: í† í¬ë‚˜ì´ì €
    
    Returns:
        list: ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    """
    keywords = []
    current_keyword = []
    previous_tag = None
    
    # í† í°ê³¼ íƒœê·¸ì˜ ê¸¸ì´ê°€ ë‹¤ë¥´ë©´ ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ
    if len(tokens) != len(bio_tags) or len(tokens) != len(attention_mask):
        print("âš ï¸ í† í°, íƒœê·¸, ë§ˆìŠ¤í¬ì˜ ê¸¸ì´ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return []
    
    # í…ì„œë¥¼ CPUì—ì„œ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ì¤€ë¹„
    tokens_cpu = tokens.cpu() if torch.is_tensor(tokens) else tokens
    bio_tags_cpu = bio_tags.cpu() if torch.is_tensor(bio_tags) else bio_tags
    attention_mask_cpu = attention_mask.cpu() if torch.is_tensor(attention_mask) else attention_mask
    
    for i, (token, tag, mask) in enumerate(zip(tokens_cpu, bio_tags_cpu, attention_mask_cpu)):
        if not mask:  # íŒ¨ë”©ëœ í† í°ì€ ê±´ë„ˆë›°ê¸°
            continue
            
        # ì •ìˆ˜ ê°’ìœ¼ë¡œ ë³€í™˜
        token_val = token.item() if torch.is_tensor(token) else token
        tag_val = tag.item() if torch.is_tensor(tag) else tag
        
        if tag_val == 0:  # B íƒœê·¸
            # ì´ì „ í‚¤ì›Œë“œê°€ ìˆì—ˆë‹¤ë©´ ì €ì¥
            if current_keyword:
                try:
                    keyword = tokenizer.decode(current_keyword).strip()
                    if keyword and keyword not in keywords:
                        keywords.append(keyword)
                except Exception as e:
                    print(f"âš ï¸ í† í° ë””ì½”ë”© ì˜¤ë¥˜: {e}")
            current_keyword = [token_val]
            previous_tag = tag_val
        elif tag_val == 1:  # I íƒœê·¸
            # B íƒœê·¸(0) ë‹¤ìŒì— ì˜¤ëŠ” I íƒœê·¸ë§Œ ì²˜ë¦¬ (O íƒœê·¸(2) ë‹¤ìŒì˜ I íƒœê·¸ëŠ” ë¬´ì‹œ)
            if previous_tag == 0 or previous_tag == 1:
                current_keyword.append(token_val)
            previous_tag = tag_val
        else:  # O íƒœê·¸(2)
            # ì´ì „ í‚¤ì›Œë“œê°€ ìˆì—ˆë‹¤ë©´ ì €ì¥
            if current_keyword:
                try:
                    keyword = tokenizer.decode(current_keyword).strip()
                    if keyword and keyword not in keywords:
                        keywords.append(keyword)
                except Exception as e:
                    print(f"âš ï¸ í† í° ë””ì½”ë”© ì˜¤ë¥˜: {e}")
            current_keyword = []
            previous_tag = tag_val
    
    # ë§ˆì§€ë§‰ í‚¤ì›Œë“œ ì²˜ë¦¬
    if current_keyword:
        try:
            keyword = tokenizer.decode(current_keyword).strip()
            if keyword and keyword not in keywords:
                keywords.append(keyword)
        except Exception as e:
            print(f"âš ï¸ í† í° ë””ì½”ë”© ì˜¤ë¥˜: {e}")
    
    return keywords


def evaluate_keywords(pred_keywords, true_keywords):
    """
    ì˜ˆì¸¡ëœ í‚¤ì›Œë“œì™€ ì‹¤ì œ í‚¤ì›Œë“œë¥¼ ë¹„êµí•˜ì—¬ TP, FP, FNì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Args:
        pred_keywords: ì˜ˆì¸¡ëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        true_keywords: ì‹¤ì œ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        tuple: (TP, FP, FN) ê°’
    """
    pred_set = set(pred_keywords)
    true_set = set(true_keywords)
    
    TP = len(pred_set & true_set)  # êµì§‘í•©
    FP = len(pred_set - true_set)  # ì˜ˆì¸¡í–ˆì§€ë§Œ ì •ë‹µì´ ì•„ë‹Œ ê²ƒ
    FN = len(true_set - pred_set)  # ì •ë‹µì´ì§€ë§Œ ì˜ˆì¸¡í•˜ì§€ ëª»í•œ ê²ƒ
    
    return TP, FP, FN


# test.pyì™€ ë™ì¼í•œ import ì¶”ê°€
from data import load_data, KeywordDataset, Collator

print("ğŸ”„ ì‹¤ì œ ëª¨ë¸ import ì‹œë„ ì¤‘...")
try:
    from model import KoKeyBERT
    print("âœ… KoKeyBERT import ì„±ê³µ")
except ImportError as e:
    print(f"âŒ KoKeyBERT import ì‹¤íŒ¨: {e}")
    # ëŒ€ì•ˆ import ì‹œë„
    try:
        sys.path.insert(0, os.path.join(grandparent_dir, 'nlp'))
        from model import KoKeyBERT
        print("âœ… KoKeyBERT import ì„±ê³µ (ëŒ€ì•ˆ ê²½ë¡œ)")
    except ImportError as e2:
        print(f"âŒ ëŒ€ì•ˆ ê²½ë¡œë„ ì‹¤íŒ¨: {e2}")
        sys.exit(1)

try:
    from distill_model import DistillKoKeyBERT
    print("âœ… DistillKoKeyBERT import ì„±ê³µ")
except ImportError as e:
    print(f"âŒ DistillKoKeyBERT import ì‹¤íŒ¨: {e}")
    sys.exit(1)

class RealDataset(Dataset):
    """ì‹¤ì œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” Dataset"""
    def __init__(self, data, tokenizer, max_length=512):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        text = item['text']
        keywords = item['keyword']
        
        # í† í¬ë‚˜ì´ì§• (íŒ¨ë”© ì—†ì´)
        encoding = self.tokenizer(
            text,
            padding=False,  # íŒ¨ë”©ì„ collate_fnì—ì„œ ì²˜ë¦¬
            truncation=True,
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        input_ids = encoding['input_ids'].squeeze(0)
        attention_mask = encoding['attention_mask'].squeeze(0)
        
        # BIO íƒœê·¸ ìƒì„± (ê°„ë‹¨í•œ ë°©ì‹)
        tags = self.create_bio_tags(text, keywords, input_ids)
        
        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'tags': tags,
            'text': text,
            'keywords': keywords
        }
    
    def create_bio_tags(self, text, keywords, input_ids):
        """í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ BIO íƒœê·¸ ìƒì„±"""
        tags = torch.full((len(input_ids),), 2, dtype=torch.long)  # ëª¨ë‘ Oë¡œ ì´ˆê¸°í™”
        
        try:
            # í† í°ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            tokens = self.tokenizer.convert_ids_to_tokens(input_ids)
            
            # ê° í‚¤ì›Œë“œì— ëŒ€í•´ BIO íƒœê·¸ ì„¤ì •
            for keyword in keywords:
                if not keyword or not isinstance(keyword, str):
                    continue
                    
                keyword_tokens = self.tokenizer.tokenize(keyword)
                if not keyword_tokens:
                    continue
                    
                # í† í° ì‹œí€€ìŠ¤ì—ì„œ í‚¤ì›Œë“œ ì°¾ê¸°
                for i in range(len(tokens) - len(keyword_tokens) + 1):
                    match = True
                    for j, kw_token in enumerate(keyword_tokens):
                        if i + j >= len(tokens) or tokens[i + j] != kw_token:
                            match = False
                            break
                    
                    if match:
                        tags[i] = 0  # B íƒœê·¸
                        for j in range(1, len(keyword_tokens)):
                            if i + j < len(tags):
                                tags[i + j] = 1  # I íƒœê·¸
                        break
        except Exception as e:
            print(f"âš ï¸ BIO íƒœê·¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ë°œìƒì‹œ ëª¨ë“  íƒœê·¸ë¥¼ Oë¡œ ì„¤ì •
            tags = torch.full((len(input_ids),), 2, dtype=torch.long)
        
        return tags

class OptimizedDistillationLoss(nn.Module):
    """ìµœì í™”ëœ Knowledge Distillation Loss"""
    def __init__(self, temperature=4.0, alpha=0.7, beta=0.2, gamma=0.1):
        super().__init__()
        self.temperature = temperature
        self.alpha = alpha  # KL Divergence ê°€ì¤‘ì¹˜
        self.beta = beta    # Task Loss ê°€ì¤‘ì¹˜  
        self.gamma = gamma  # Cosine Loss ê°€ì¤‘ì¹˜
        
        self.kl_div = nn.KLDivLoss(reduction='batchmean')
        self.mse_loss = nn.MSELoss()
        self.cosine_loss = nn.CosineEmbeddingLoss()
        
    def forward(self, student_logits, teacher_logits, student_hidden, teacher_hidden, attention_mask):
        total_loss = 0.0
        loss_components = {}
        
        # 1. KL Divergence Loss (Knowledge Distillation)
        try:
            student_soft = F.log_softmax(student_logits / self.temperature, dim=-1)
            teacher_soft = F.softmax(teacher_logits / self.temperature, dim=-1)
            
            # ë§ˆìŠ¤í‚¹
            mask = attention_mask.unsqueeze(-1).expand_as(student_soft).bool()
            student_masked = student_soft[mask].view(-1, student_logits.size(-1))
            teacher_masked = teacher_soft[mask].view(-1, teacher_logits.size(-1))
            
            if student_masked.size(0) > 0:
                kl_loss = self.kl_div(student_masked, teacher_masked) * (self.temperature ** 2)
                total_loss += self.alpha * kl_loss
                loss_components['kl_loss'] = kl_loss.item()
            else:
                loss_components['kl_loss'] = 0.0
                
        except Exception as e:
            print(f"âš ï¸ KL Loss ê³„ì‚° ì˜¤ë¥˜: {e}")
            loss_components['kl_loss'] = 0.0
        
        # 2. Task Consistency Loss
        try:
            student_probs = F.softmax(student_logits, dim=-1)
            teacher_probs = F.softmax(teacher_logits, dim=-1)
            
            # Confidence consistency
            student_conf = student_probs.max(dim=-1)[0]
            teacher_conf = teacher_probs.max(dim=-1)[0]
            
            valid_mask = attention_mask.bool()
            if valid_mask.sum() > 0:
                task_loss = self.mse_loss(student_conf[valid_mask], teacher_conf[valid_mask])
                total_loss += self.beta * task_loss
                loss_components['task_loss'] = task_loss.item()
            else:
                loss_components['task_loss'] = 0.0
                
        except Exception as e:
            print(f"âš ï¸ Task Loss ê³„ì‚° ì˜¤ë¥˜: {e}")
            loss_components['task_loss'] = 0.0
        
        # 3. Hidden State Alignment (Cosine Loss)
        try:
            # ì°¨ì› ë§ì¶”ê¸°
            if student_hidden.size(-1) != teacher_hidden.size(-1):
                # Student hiddenì„ Teacher ì°¨ì›ìœ¼ë¡œ projection
                projection = nn.Linear(student_hidden.size(-1), teacher_hidden.size(-1)).to(student_hidden.device)
                student_hidden_proj = projection(student_hidden)
            else:
                student_hidden_proj = student_hidden
            
            # ë§ˆìŠ¤í‚¹ëœ hidden states
            mask_flat = attention_mask.view(-1).bool()
            student_flat = student_hidden_proj.view(-1, student_hidden_proj.size(-1))[mask_flat]
            teacher_flat = teacher_hidden.view(-1, teacher_hidden.size(-1))[mask_flat]
            
            if student_flat.size(0) > 0:
                target = torch.ones(student_flat.size(0)).to(student_flat.device)
                cosine_loss = self.cosine_loss(student_flat, teacher_flat, target)
                total_loss += self.gamma * cosine_loss
                loss_components['cosine_loss'] = cosine_loss.item()
            else:
                loss_components['cosine_loss'] = 0.0
                
        except Exception as e:
            print(f"âš ï¸ Cosine Loss ê³„ì‚° ì˜¤ë¥˜: {e}")
            loss_components['cosine_loss'] = 0.0
        
        loss_components['total_loss'] = total_loss.item() if isinstance(total_loss, torch.Tensor) else total_loss
        
        return total_loss, loss_components

def load_real_data(data_type='train', max_samples=None):
    """ì‹¤ì œ ë°ì´í„° ë¡œë“œ"""
    print(f"ğŸ“Š ì‹¤ì œ {data_type} ë°ì´í„° ë¡œë”© ì¤‘...")
    
    # ë°ì´í„° ê²½ë¡œë“¤
    data_paths = [
        f"../../src/data/{data_type}_clean.json",
        f"../../../src/data/{data_type}_clean.json",
        f"../../../../src/data/{data_type}_clean.json"
    ]
    
    for path in data_paths:
        if os.path.exists(path):
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                if isinstance(data, dict):
                    data = list(data.values())
                
                print(f"âœ… ì‹¤ì œ {data_type} ë°ì´í„° ë¡œë“œ: {len(data):,}ê°œ")
                print(f"ğŸ“Š ë°ì´í„° êµ¬ì¡°: {type(data)} í˜•íƒœ, í‚¤ ê°œìˆ˜: {len(data)}ê°œ")
                
                # ìƒ˜í”Œë§ (ì˜µì…˜)
                if max_samples and len(data) > max_samples:
                    data = random.sample(data, max_samples)
                    print(f"ğŸ“‰ ìƒ˜í”Œë§: {len(data):,}ê°œ")
                
                return data
                
            except Exception as e:
                print(f"âš ï¸ {path} ë¡œë“œ ì‹¤íŒ¨: {e}")
                continue
    
    raise FileNotFoundError(f"âŒ {data_type} ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìŒ ê²½ë¡œë“¤ì„ í™•ì¸í•˜ì„¸ìš”: {data_paths}")

def create_real_models(device):
    """ì‹¤ì œ KoKeyBERT ëª¨ë¸ë“¤ ìƒì„±"""
    print("ğŸ—ï¸ ì‹¤ì œ ëª¨ë¸ë“¤ ìƒì„± ì¤‘...")
    
            # Teacher ëª¨ë¸ (ì‚¬ì „ í›ˆë ¨ëœ KoKeyBERT ë¡œë“œ)
    print("ğŸ“š Teacher ëª¨ë¸ (KoKeyBERT) ìƒì„± ì¤‘...")
    try:
        teacher_model = KoKeyBERT(num_class=3, model_name='skt/kobert-base-v1')
        print("âœ… KoBERT ê¸°ë°˜ Teacher ëª¨ë¸ ìƒì„± ì„±ê³µ")
        
        # ì‚¬ì „ í›ˆë ¨ëœ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        checkpoint_path = "../../best_model.pt"
        if os.path.exists(checkpoint_path):
            print(f"ğŸ”„ ì‚¬ì „ í›ˆë ¨ëœ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì¤‘: {checkpoint_path}")
            teacher_model.load_state_dict(torch.load(checkpoint_path, map_location=device))
            print("âœ… ì‚¬ì „ í›ˆë ¨ëœ Teacher ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        else:
            print("âš ï¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëœë¤ ì´ˆê¸°í™”ëœ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            
    except Exception as e:
        print(f"âš ï¸ KoBERT ê¸°ë°˜ ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
        print("ğŸ”„ ê¸°ë³¸ BERT ê¸°ë°˜ ëª¨ë¸ë¡œ ëŒ€ì²´...")
        teacher_model = KoKeyBERT(num_class=3, model_name='bert-base-uncased')
        print("âœ… ê¸°ë³¸ BERT ê¸°ë°˜ Teacher ëª¨ë¸ ìƒì„± ì„±ê³µ")
    
    teacher_model = teacher_model.to(device)
    teacher_params = sum(p.numel() for p in teacher_model.parameters())
    print(f"âœ… Teacher ëª¨ë¸: {teacher_params:,} íŒŒë¼ë¯¸í„°")
    
    # Student ëª¨ë¸ (DistillKoKeyBERT) - ì‘ì€ í¬ê¸°ë¡œ ìƒì„±
    print("ğŸ“ Student ëª¨ë¸ (DistillKoKeyBERT) ìƒì„± ì¤‘...")
    try:
        # DistilBERT ìŠ¤íƒ€ì¼: Teacherì˜ ì ˆë°˜ í¬ê¸°
        from transformers import BertConfig
        
        # Teacherì˜ configë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ì€ Student config ìƒì„±
        teacher_config = teacher_model.config
        student_config = BertConfig(
            vocab_size=teacher_config.vocab_size,
            hidden_size=teacher_config.hidden_size,
            num_hidden_layers=teacher_config.num_hidden_layers // 2,  # ì ˆë°˜ ë ˆì´ì–´
            num_attention_heads=teacher_config.num_attention_heads,
            intermediate_size=teacher_config.intermediate_size,
            hidden_dropout_prob=teacher_config.hidden_dropout_prob,
            attention_probs_dropout_prob=teacher_config.attention_probs_dropout_prob,
            max_position_embeddings=teacher_config.max_position_embeddings,
            type_vocab_size=teacher_config.type_vocab_size,
            initializer_range=teacher_config.initializer_range,
            layer_norm_eps=teacher_config.layer_norm_eps,
            pad_token_id=teacher_config.pad_token_id,
            bos_token_id=teacher_config.bos_token_id,
            eos_token_id=teacher_config.eos_token_id,
            use_cache=teacher_config.use_cache,
        )
        
        student_model = DistillKoKeyBERT(config=student_config, num_class=3)
        print("âœ… ì‘ì€ í¬ê¸° Student ëª¨ë¸ ìƒì„± ì„±ê³µ")
        print(f"   - Teacher ë ˆì´ì–´: {teacher_config.num_hidden_layers}ê°œ")
        print(f"   - Student ë ˆì´ì–´: {student_config.num_hidden_layers}ê°œ")
        
    except Exception as e:
        print(f"âš ï¸ ì‘ì€ í¬ê¸° Student ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
        print("ğŸ”„ ê¸°ë³¸ í¬ê¸° Student ëª¨ë¸ë¡œ ëŒ€ì²´...")
        student_model = DistillKoKeyBERT(num_class=3, model_name='skt/kobert-base-v1')
        print("âœ… ê¸°ë³¸ í¬ê¸° Student ëª¨ë¸ ìƒì„± ì„±ê³µ")
    
    # DistilBERT ìŠ¤íƒ€ì¼ ì´ˆê¸°í™”: Teacherì˜ 2ë²ˆì§¸ ë ˆì´ì–´ì”© ë³µì‚¬
    print("ğŸ”„ DistilBERT ìŠ¤íƒ€ì¼ ì´ˆê¸°í™” ì¤‘...")
    try:
        # Teacherì™€ Studentì˜ encoder ë ˆì´ì–´ ìˆ˜ í™•ì¸
        teacher_layers = len(teacher_model.model.encoder.layer)
        student_layers = len(student_model.model.encoder.layer)
        
        print(f"   - Teacher encoder ë ˆì´ì–´: {teacher_layers}ê°œ")
        print(f"   - Student encoder ë ˆì´ì–´: {student_layers}ê°œ")
        
        # 2ë²ˆì§¸ ë ˆì´ì–´ì”© ë³µì‚¬ (DistilBERT ë°©ì‹)
        for i in range(student_layers):
            teacher_layer_idx = i * 2  # 0, 2, 4, 6, ...
            if teacher_layer_idx < teacher_layers:
                # Studentì˜ ië²ˆì§¸ ë ˆì´ì–´ì— Teacherì˜ 2*ië²ˆì§¸ ë ˆì´ì–´ ë³µì‚¬
                student_model.model.encoder.layer[i].load_state_dict(
                    teacher_model.model.encoder.layer[teacher_layer_idx].state_dict()
                )
                print(f"   - Student layer {i} â† Teacher layer {teacher_layer_idx}")
            else:
                print(f"   - Student layer {i}: ëœë¤ ì´ˆê¸°í™” (Teacher layer {teacher_layer_idx} ì—†ìŒ)")
        
        # Embedding ë ˆì´ì–´ë„ ë³µì‚¬
        student_model.model.embeddings.load_state_dict(
            teacher_model.model.embeddings.state_dict()
        )
        print("   - Embedding ë ˆì´ì–´ ë³µì‚¬ ì™„ë£Œ")
        
        print("âœ… DistilBERT ìŠ¤íƒ€ì¼ ì´ˆê¸°í™” ì™„ë£Œ")
        
    except Exception as e:
        print(f"âš ï¸ DistilBERT ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        print("   - Student ëª¨ë¸ì€ ëœë¤ ì´ˆê¸°í™” ìƒíƒœë¡œ ìœ ì§€")
    
    student_model = student_model.to(device)
    student_params = sum(p.numel() for p in student_model.parameters())
    print(f"âœ… Student ëª¨ë¸: {student_params:,} íŒŒë¼ë¯¸í„°")
    
    compression_ratio = student_params / teacher_params
    print(f"ğŸ“Š ì••ì¶• ë¹„ìœ¨: {compression_ratio:.3f}")
    
    return teacher_model, student_model

def train_real_distillation(teacher_model, student_model, train_loader, test_loader, device, tokenizer, num_epochs=5):
    """ì‹¤ì œ ë°ì´í„°ë¡œ Knowledge Distillation í›ˆë ¨"""
    print("ğŸš€ ì‹¤ì œ Knowledge Distillation í›ˆë ¨ ì‹œì‘!")
    
    # ì„¤ì •
    learning_rate = 2e-5
    
    # Lossì™€ Optimizer
    criterion = OptimizedDistillationLoss()
    optimizer = torch.optim.AdamW(student_model.parameters(), lr=learning_rate)
    
    # Best Model ì¶”ì  (ê²€ì¦ ì •í™•ë„ ê¸°ì¤€)
    best_val_acc = 0.0
    best_model_state = None
    patience = 3  # Early stopping patience
    patience_counter = 0
    
    # í›ˆë ¨ ëª¨ë“œ ì„¤ì •
    teacher_model.eval()  # TeacherëŠ” frozen
    student_model.train()
    
    training_history = []
    
    for epoch in range(num_epochs):
        epoch_start = datetime.now()
        epoch_losses = []
        epoch_components = {'kl_loss': [], 'task_loss': [], 'cosine_loss': []}
        
        print(f"\nğŸ“– Epoch {epoch+1}/{num_epochs}")
        
        for batch_idx, batch in enumerate(train_loader):
            # test.py ë°©ì‹ì˜ batch í˜•ì‹: (index, input_ids, attention_mask, tags)
            index, input_ids, attention_mask, tags = batch
            input_ids = input_ids.to(device)
            attention_mask = attention_mask.to(device)
            tags = tags.to(device)
            
            optimizer.zero_grad()
            
            # Teacher ì˜ˆì¸¡ (frozen) - KoKeyBERT ì •ìƒ í˜¸ì¶œ
            with torch.no_grad():
                teacher_output = teacher_model(input_ids, attention_mask, tags)
                # KoKeyBERT ì¶œë ¥: (log_likelihood, sequence_of_tags)
                if isinstance(teacher_output, tuple):
                    teacher_log_likelihood, teacher_sequence = teacher_output
                    # Hidden states ê°€ì ¸ì˜¤ê¸° (Knowledge Distillationìš©)
                    teacher_bert_outputs = teacher_model.model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)
                    teacher_hidden = teacher_bert_outputs.last_hidden_state
                    teacher_logits = teacher_model.classifier(teacher_model.dropout(teacher_hidden))
                else:
                    teacher_sequence = teacher_output
                    teacher_bert_outputs = teacher_model.model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)
                    teacher_hidden = teacher_bert_outputs.last_hidden_state
                    teacher_logits = teacher_model.classifier(teacher_model.dropout(teacher_hidden))
                
            # Student ì˜ˆì¸¡ - DistillKoKeyBERT ì •ìƒ í˜¸ì¶œ
            student_output = student_model(input_ids, attention_mask, tags, return_outputs=True)
            # DistillKoKeyBERT ì¶œë ¥: (loss, predicted_tags, bert_outputs) ë˜ëŠ” (predicted_tags, bert_outputs)
            if len(student_output) == 3:
                # í›ˆë ¨ ëª¨ë“œ: (loss, predicted_tags, bert_outputs)
                student_loss, student_predictions, student_bert_outputs = student_output
                student_hidden = student_bert_outputs[0]  # last_hidden_state
                student_logits = student_model.classifier(student_model.dropout(student_hidden))
            else:
                # ì¶”ë¡  ëª¨ë“œ: (predicted_tags, bert_outputs)
                student_predictions, student_bert_outputs = student_output
                student_hidden = student_bert_outputs[0]  # last_hidden_state
                student_logits = student_model.classifier(student_model.dropout(student_hidden))
            
            # ë””ë²„ê¹…: ì²« ë²ˆì§¸ ë°°ì¹˜ì—ì„œë§Œ ì¶œë ¥ í˜•íƒœ í™•ì¸
            if batch_idx == 0:
                print(f"ğŸ” Teacher hidden states í˜•íƒœ: {teacher_hidden.shape}")
                print(f"ğŸ” Teacher logits í˜•íƒœ: {teacher_logits.shape}")
                print(f"ğŸ” Student hidden states í˜•íƒœ: {student_hidden.shape}")
                print(f"ğŸ” Student logits í˜•íƒœ: {student_logits.shape}")
            

            
            # Loss ê³„ì‚°
            total_loss, loss_components = criterion(
                student_logits, teacher_logits, 
                student_hidden, teacher_hidden, 
                attention_mask
            )
            
            # Backward pass
            if isinstance(total_loss, torch.Tensor) and total_loss.requires_grad:
                total_loss.backward()
                torch.nn.utils.clip_grad_norm_(student_model.parameters(), 1.0)
                optimizer.step()
                
                # ì†ì‹¤ ê¸°ë¡
                epoch_losses.append(loss_components['total_loss'])
                for key in epoch_components:
                    if key in loss_components:
                        epoch_components[key].append(loss_components[key])
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥ (ë” ìì£¼ ì¶œë ¥)
            if batch_idx % 20 == 0:
                print(f"  Batch {batch_idx:3d}: Total={loss_components['total_loss']:.4f}, "
                      f"KL={loss_components['kl_loss']:.4f}, "
                      f"Task={loss_components['task_loss']:.4f}, "
                      f"Cosine={loss_components['cosine_loss']:.4f}")
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (ë” ìì£¼)
            if batch_idx % 50 == 0:
                torch.cuda.empty_cache()
        
        # Epoch ê²°ê³¼
        avg_loss = np.mean(epoch_losses) if epoch_losses else 0
        epoch_time = datetime.now() - epoch_start
        
        # ê²€ì¦ ë©”íŠ¸ë¦­ ê³„ì‚° (ë¹ ë¥¸ í‰ê°€) - 2ì—í­ë§ˆë‹¤ë§Œ ì‹¤í–‰
        if epoch % 2 == 0:
            val_metrics = evaluate_epoch_accuracy(student_model, test_loader, device, tokenizer, max_batches=64)
            val_f1 = val_metrics['f1']
            val_precision = val_metrics['precision']
            val_recall = val_metrics['recall']
            val_accuracy = val_metrics['accuracy']
        else:
            val_f1 = 0.0  # ê²€ì¦ ê±´ë„ˆë›°ê¸°
            val_precision = 0.0
            val_recall = 0.0
            val_accuracy = 0.0
        
        # Best Model ì²´í¬ (ê²€ì¦ F1 ì ìˆ˜ ê¸°ì¤€)
        if val_f1 > best_val_acc:
            best_val_acc = val_f1
            best_model_state = student_model.state_dict().copy()
            patience_counter = 0
            print(f"ğŸ† ìƒˆë¡œìš´ Best Model! Val F1: {best_val_acc:.4f}")
        else:
            patience_counter += 1
            print(f"â³ Best Model ì—†ìŒ. Patience: {patience_counter}/{patience}")
        
        epoch_summary = {
            'epoch': epoch + 1,
            'avg_loss': avg_loss,
            'val_f1': val_f1,
            'val_precision': val_precision,
            'val_recall': val_recall,
            'val_accuracy': val_accuracy,
            'best_val_f1': best_val_acc,
            'time': epoch_time,
            'components': {k: np.mean(v) if v else 0 for k, v in epoch_components.items()}
        }
        training_history.append(epoch_summary)
        
        print(f"âœ… Epoch {epoch+1} ì™„ë£Œ:")
        print(f"   í‰ê·  Loss: {avg_loss:.4f}")
        if epoch % 2 == 0:  # ë©”íŠ¸ë¦­ì´ ê³„ì‚°ëœ ê²½ìš°ì—ë§Œ ì¶œë ¥
            print(f"   ğŸ“ˆ F1: {val_f1:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, Accuracy: {val_accuracy:.4f}")
        print(f"   Best Val F1: {best_val_acc:.4f}")
        print(f"   KL Loss: {epoch_summary['components']['kl_loss']:.4f}")
        print(f"   Task Loss: {epoch_summary['components']['task_loss']:.4f}")
        print(f"   Cosine Loss: {epoch_summary['components']['cosine_loss']:.4f}")
        print(f"   ì†Œìš” ì‹œê°„: {epoch_time}")
        
        # Early Stopping ì²´í¬
        if patience_counter >= patience:
            print(f"ğŸ›‘ Early Stopping! {patience} epochs ë™ì•ˆ ê°œì„  ì—†ìŒ")
            break
    
    # í›ˆë ¨ ì™„ë£Œ í›„ Best Model ë³µì›
    if best_model_state is not None:
        student_model.load_state_dict(best_model_state)
        print(f"ğŸ† Best Model ë³µì› ì™„ë£Œ (Val Acc: {best_val_acc:.4f})")
    
    return training_history, best_model_state, best_val_acc

def evaluate_epoch_accuracy(model, test_loader, device, tokenizer, max_batches=64):
    """Epoch ì¤‘ê°„ì— ë¹ ë¥¸ ê²€ì¦ F1 ì ìˆ˜ ê³„ì‚° (í‚¤ì›Œë“œ ê¸°ë°˜)"""
    model.eval()
    
    # í‚¤ì›Œë“œ í‰ê°€ë¥¼ ìœ„í•œ í†µê³„
    total_TP = 0
    total_FP = 0
    total_FN = 0
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(test_loader):
            if batch_idx >= max_batches:  # ë¹ ë¥¸ í‰ê°€ë¥¼ ìœ„í•´ ì œí•œ
                break
                
            # test.py ë°©ì‹ì˜ batch í˜•ì‹: (index, input_ids, attention_mask, tags)
            index, input_ids, attention_mask, tags = batch
            input_ids = input_ids.to(device)
            attention_mask = attention_mask.to(device)
            tags = tags.to(device)
            
            # ì˜ˆì¸¡
            model_output = model(input_ids, attention_mask, tags)
            
            # KoKeyBERT ëª¨ë¸ ì¶œë ¥ ì²˜ë¦¬: (log_likelihood, sequence_of_tags)
            if isinstance(model_output, tuple):
                log_likelihood, sequence_of_tags = model_output
                
                # ë””ë²„ê¹…: ì²« ë²ˆì§¸ ë°°ì¹˜ì—ì„œë§Œ ì¶œë ¥ í˜•íƒœ í™•ì¸
                if batch_idx == 0:
                    print(f"ğŸ” Model output type: {type(model_output)}")
                    print(f"ğŸ” sequence_of_tags length: {len(sequence_of_tags)}")
                    print(f"ğŸ” Tags shape: {tags.shape}")
                    print(f"ğŸ” Attention mask shape: {attention_mask.shape}")
                
                # sequence_of_tagsë¥¼ í…ì„œë¡œ ë³€í™˜
                if isinstance(sequence_of_tags, list):
                    batch_size = tags.size(0)
                    seq_len = tags.size(1)
                    predictions = torch.zeros(batch_size, seq_len, dtype=torch.long, device=tags.device)
                    
                    for i, seq in enumerate(sequence_of_tags):
                        if i < batch_size and len(seq) <= seq_len:
                            predictions[i, :len(seq)] = torch.tensor(seq, dtype=torch.long, device=tags.device)
                else:
                    predictions = sequence_of_tags
            else:
                # ë‹¨ì¼ ì¶œë ¥ì¸ ê²½ìš° ì²˜ë¦¬
                if isinstance(model_output, list):
                    batch_size = tags.size(0)
                    seq_len = tags.size(1)
                    predictions = torch.zeros(batch_size, seq_len, dtype=torch.long, device=tags.device)
                    
                    for i, seq in enumerate(model_output):
                        if i < batch_size and len(seq) <= seq_len:
                            predictions[i, :len(seq)] = torch.tensor(seq, dtype=torch.long, device=tags.device)
                else:
                    predictions = model_output
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ ë° í‰ê°€
            batch_TP = 0
            batch_FP = 0
            batch_FN = 0
            
            for i in range(input_ids.size(0)):  # ë°°ì¹˜ì˜ ê° ìƒ˜í”Œì— ëŒ€í•´
                try:
                    # ì˜ˆì¸¡ëœ í‚¤ì›Œë“œ ì¶”ì¶œ
                    pred_keywords = extract_keywords_from_bio_tags(
                        input_ids[i],
                        predictions[i],
                        attention_mask[i],
                        tokenizer
                    )
                    
                    # ì‹¤ì œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì •ë‹µ BIO íƒœê·¸ì—ì„œ)
                    true_keywords = extract_keywords_from_bio_tags(
                        input_ids[i],
                        tags[i],
                        attention_mask[i],
                        tokenizer
                    )
                    
                    # ë””ë²„ê¹…: ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ ì²« ë²ˆì§¸ ìƒ˜í”Œì—ì„œ í‚¤ì›Œë“œ í™•ì¸
                    if batch_idx == 0 and i == 0:
                        print(f"ğŸ” ë””ë²„ê¹… - ì²« ë²ˆì§¸ ìƒ˜í”Œ:")
                        print(f"   ì˜ˆì¸¡ í‚¤ì›Œë“œ: {pred_keywords}")
                        print(f"   ì‹¤ì œ í‚¤ì›Œë“œ: {true_keywords}")
                        print(f"   BIO íƒœê·¸ (ì˜ˆì¸¡): {predictions[i][:20].tolist()}")  # ì²˜ìŒ 20ê°œë§Œ
                        print(f"   BIO íƒœê·¸ (ì‹¤ì œ): {tags[i][:20].tolist()}")  # ì²˜ìŒ 20ê°œë§Œ
                        print(f"   í† í° ID: {input_ids[i][:20].tolist()}")  # ì²˜ìŒ 20ê°œë§Œ
                        
                        # í† í° ë””ì½”ë”©ë„ í™•ì¸
                        tokens_text = tokenizer.decode(input_ids[i][:20], skip_special_tokens=True)
                        print(f"   í† í° í…ìŠ¤íŠ¸: {tokens_text}")
                    
                    # TP, FP, FN ê³„ì‚°
                    TP, FP, FN = evaluate_keywords(pred_keywords, true_keywords)
                    batch_TP += TP
                    batch_FP += FP
                    batch_FN += FN
                    
                except Exception as e:
                    if batch_idx == 0:  # ì²« ë°°ì¹˜ì—ì„œë§Œ ì˜¤ë¥˜ ì¶œë ¥
                        print(f"âš ï¸  í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜ (ìƒ˜í”Œ {i}): {e}")
                    continue
            
            total_TP += batch_TP
            total_FP += batch_FP
            total_FN += batch_FN
    
    model.train()  # ë‹¤ì‹œ í›ˆë ¨ ëª¨ë“œë¡œ
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    if total_TP + total_FP > 0:
        precision = total_TP / (total_TP + total_FP)
    else:
        precision = 0.0
        
    if total_TP + total_FN > 0:
        recall = total_TP / (total_TP + total_FN)
    else:
        recall = 0.0
        
    if precision + recall > 0:
        f1 = 2 * (precision * recall) / (precision + recall)
    else:
        f1 = 0.0
    
    # Accuracy ê³„ì‚° (í‚¤ì›Œë“œ ê¸°ë°˜)
    total_predictions = total_TP + total_FP
    total_actual = total_TP + total_FN
    if total_predictions + total_actual > 0:
        accuracy = total_TP / max(total_predictions, total_actual) if max(total_predictions, total_actual) > 0 else 0.0
    else:
        accuracy = 0.0
    
    return {
        'f1': f1,
        'precision': precision,
        'recall': recall,
        'accuracy': accuracy,
        'tp': total_TP,
        'fp': total_FP,
        'fn': total_FN
    }

def evaluate_real_models(teacher_model, student_model, test_loader, device, tokenizer):
    """ì‹¤ì œ ë°ì´í„°ë¡œ ëª¨ë¸ í‰ê°€"""
    print("ğŸ” ì‹¤ì œ ë°ì´í„°ë¡œ ëª¨ë¸ í‰ê°€ ì¤‘...")
    
    teacher_model.eval()
    student_model.eval()
    
    # í‚¤ì›Œë“œ í‰ê°€ë¥¼ ìœ„í•œ í†µê³„
    teacher_TP = 0
    teacher_FP = 0
    teacher_FN = 0
    
    student_TP = 0
    student_FP = 0
    student_FN = 0
    
    teacher_times = []
    student_times = []
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(test_loader):
            # test.py ë°©ì‹ì˜ batch í˜•ì‹: (index, input_ids, attention_mask, tags)
            index, input_ids, attention_mask, tags = batch
            input_ids = input_ids.to(device)
            attention_mask = attention_mask.to(device)
            tags = tags.to(device)
            
            # Teacher í‰ê°€
            start_time = time.time()
            teacher_output = teacher_model(input_ids, attention_mask, tags)
            teacher_time = time.time() - start_time
            teacher_times.append(teacher_time)
            
            # Student í‰ê°€
            start_time = time.time()
            student_output = student_model(input_ids, attention_mask, tags)
            student_time = time.time() - start_time
            student_times.append(student_time)
            
            # KoKeyBERT ëª¨ë¸ ì¶œë ¥ ì²˜ë¦¬: (log_likelihood, sequence_of_tags)
            def process_model_output(model_output, model_name="Model"):
                if isinstance(model_output, tuple):
                    log_likelihood, sequence_of_tags = model_output
                    
                    # sequence_of_tagsë¥¼ í…ì„œë¡œ ë³€í™˜
                    if isinstance(sequence_of_tags, list):
                        batch_size = tags.size(0)
                        seq_len = tags.size(1)
                        predictions = torch.zeros(batch_size, seq_len, dtype=torch.long, device=tags.device)
                        
                        for i, seq in enumerate(sequence_of_tags):
                            if i < batch_size and len(seq) <= seq_len:
                                predictions[i, :len(seq)] = torch.tensor(seq, dtype=torch.long, device=tags.device)
                        return predictions
                    else:
                        return sequence_of_tags
                else:
                    # ë‹¨ì¼ ì¶œë ¥ì¸ ê²½ìš° (sequence_of_tagsë§Œ ë°˜í™˜)
                    if isinstance(model_output, list):
                        batch_size = tags.size(0)
                        seq_len = tags.size(1)
                        predictions = torch.zeros(batch_size, seq_len, dtype=torch.long, device=tags.device)
                        
                        for i, seq in enumerate(model_output):
                            if i < batch_size and len(seq) <= seq_len:
                                predictions[i, :len(seq)] = torch.tensor(seq, dtype=torch.long, device=tags.device)
                        return predictions
                    else:
                        return model_output
            
            teacher_pred = process_model_output(teacher_output, "Teacher")
            student_pred = process_model_output(student_output, "Student")
            
            # ë””ë²„ê¹…: ì²« ë²ˆì§¸ ë°°ì¹˜ì—ì„œë§Œ ì¶œë ¥ í˜•íƒœ í™•ì¸
            if batch_idx == 0:
                print(f"ğŸ” Teacher output type: {type(teacher_output)}")
                print(f"ğŸ” Student output type: {type(student_output)}")
                print(f"ğŸ” Teacher pred shape: {teacher_pred.shape if hasattr(teacher_pred, 'shape') else 'no shape'}")
                print(f"ğŸ” Student pred shape: {student_pred.shape if hasattr(student_pred, 'shape') else 'no shape'}")
                print(f"ğŸ” Tags shape: {tags.shape}")
            
            # predictionsê°€ ì˜¬ë°”ë¥¸ í˜•íƒœì¸ì§€ í™•ì¸
            if not isinstance(teacher_pred, torch.Tensor) or not isinstance(student_pred, torch.Tensor):
                print(f"âš ï¸  Warning: predictions are not tensors, skipping batch {batch_idx}")
                continue
                
            if teacher_pred.dim() != 2 or student_pred.dim() != 2:
                print(f"âš ï¸  Warning: predictions have unexpected dimensions, skipping batch {batch_idx}")
                continue
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ ë° í‰ê°€ (ë°°ì¹˜ì˜ ê° ìƒ˜í”Œì— ëŒ€í•´)
            for i in range(input_ids.size(0)):
                try:
                    # ì‹¤ì œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì •ë‹µ BIO íƒœê·¸ì—ì„œ)
                    true_keywords = extract_keywords_from_bio_tags(
                        input_ids[i],
                        tags[i],
                        attention_mask[i],
                        tokenizer
                    )
                    
                    # Teacher ì˜ˆì¸¡ í‚¤ì›Œë“œ ì¶”ì¶œ
                    teacher_keywords = extract_keywords_from_bio_tags(
                        input_ids[i],
                        teacher_pred[i],
                        attention_mask[i],
                        tokenizer
                    )
                    
                    # Student ì˜ˆì¸¡ í‚¤ì›Œë“œ ì¶”ì¶œ
                    student_keywords = extract_keywords_from_bio_tags(
                        input_ids[i],
                        student_pred[i],
                        attention_mask[i],
                        tokenizer
                    )
                    
                    # Teacher ë©”íŠ¸ë¦­ ê³„ì‚°
                    teacher_tp, teacher_fp, teacher_fn = evaluate_keywords(teacher_keywords, true_keywords)
                    teacher_TP += teacher_tp
                    teacher_FP += teacher_fp
                    teacher_FN += teacher_fn
                    
                    # Student ë©”íŠ¸ë¦­ ê³„ì‚°
                    student_tp, student_fp, student_fn = evaluate_keywords(student_keywords, true_keywords)
                    student_TP += student_tp
                    student_FP += student_fp
                    student_FN += student_fn
                    
                except Exception as e:
                    if batch_idx == 0:  # ì²« ë°°ì¹˜ì—ì„œë§Œ ì˜¤ë¥˜ ì¶œë ¥
                        print(f"âš ï¸  í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜ (ë°°ì¹˜ {batch_idx}, ìƒ˜í”Œ {i}): {e}")
                    continue
            
            if batch_idx >= 100:  # ì²˜ìŒ 100 ë°°ì¹˜ë§Œ í‰ê°€ (ì‹œê°„ ì ˆì•½)
                break
    
    # Teacher ë©”íŠ¸ë¦­ ê³„ì‚°
    if teacher_TP + teacher_FP > 0:
        teacher_precision = teacher_TP / (teacher_TP + teacher_FP)
    else:
        teacher_precision = 0.0
        
    if teacher_TP + teacher_FN > 0:
        teacher_recall = teacher_TP / (teacher_TP + teacher_FN)
    else:
        teacher_recall = 0.0
        
    if teacher_precision + teacher_recall > 0:
        teacher_f1 = 2 * (teacher_precision * teacher_recall) / (teacher_precision + teacher_recall)
    else:
        teacher_f1 = 0.0
    
    teacher_total_predictions = teacher_TP + teacher_FP
    teacher_total_actual = teacher_TP + teacher_FN
    if teacher_total_predictions + teacher_total_actual > 0:
        teacher_acc = teacher_TP / max(teacher_total_predictions, teacher_total_actual) if max(teacher_total_predictions, teacher_total_actual) > 0 else 0.0
    else:
        teacher_acc = 0.0
    
    # Student ë©”íŠ¸ë¦­ ê³„ì‚°
    if student_TP + student_FP > 0:
        student_precision = student_TP / (student_TP + student_FP)
    else:
        student_precision = 0.0
        
    if student_TP + student_FN > 0:
        student_recall = student_TP / (student_TP + student_FN)
    else:
        student_recall = 0.0
        
    if student_precision + student_recall > 0:
        student_f1 = 2 * (student_precision * student_recall) / (student_precision + student_recall)
    else:
        student_f1 = 0.0
    
    student_total_predictions = student_TP + student_FP
    student_total_actual = student_TP + student_FN
    if student_total_predictions + student_total_actual > 0:
        student_acc = student_TP / max(student_total_predictions, student_total_actual) if max(student_total_predictions, student_total_actual) > 0 else 0.0
    else:
        student_acc = 0.0
    
    avg_teacher_time = np.mean(teacher_times) * 1000  # ms
    avg_student_time = np.mean(student_times) * 1000  # ms
    speedup = avg_teacher_time / avg_student_time if avg_student_time > 0 else 1.0
    
    results = {
        'teacher': {
            'accuracy': teacher_acc,
            'precision': teacher_precision,
            'recall': teacher_recall,
            'f1_score': teacher_f1,
            'tp': teacher_TP,
            'fp': teacher_FP,
            'fn': teacher_FN,
            'avg_time_ms': avg_teacher_time
        },
        'student': {
            'accuracy': student_acc,
            'precision': student_precision,
            'recall': student_recall,
            'f1_score': student_f1,
            'tp': student_TP,
            'fp': student_FP,
            'fn': student_FN,
            'avg_time_ms': avg_student_time
        },
        'speedup': speedup
    }
    
    return results

def save_real_results(teacher_model, student_model, training_history, eval_results, device, best_val_acc):
    """ì‹¤ì œ ì‹¤í—˜ ê²°ê³¼ ì €ì¥"""
    print("ğŸ’¾ ì‹¤ì œ ì‹¤í—˜ ê²°ê³¼ ì €ì¥ ì¤‘...")
    
    # ëª¨ë¸ ì €ì¥
    os.makedirs('../results/models', exist_ok=True)
    
    # Student ëª¨ë¸ë§Œ ì €ì¥ (TeacherëŠ” ì›ë³¸ì´ë¯€ë¡œ)
    torch.save({
        'model_state_dict': student_model.state_dict(),
        'model_config': student_model.config,
        'training_history': training_history,
        'eval_results': eval_results,
        'best_val_acc': best_val_acc
    }, '../results/models/real_distilled_koKeyBERT.pt')
    print("âœ… Student ëª¨ë¸ ì €ì¥ ì™„ë£Œ")
    
    # ê²°ê³¼ ì‹œê°í™”
    os.makedirs('../results/plots', exist_ok=True)
    
    plt.figure(figsize=(15, 5))
    
    # 1. í›ˆë ¨ Loss ê·¸ë˜í”„
    plt.subplot(1, 3, 1)
    epochs = [h['epoch'] for h in training_history]
    losses = [h['avg_loss'] for h in training_history]
    kl_losses = [h['components']['kl_loss'] for h in training_history]
    task_losses = [h['components']['task_loss'] for h in training_history]
    cosine_losses = [h['components']['cosine_loss'] for h in training_history]
    
    plt.plot(epochs, losses, 'b-', label='Total Loss', marker='o')
    plt.plot(epochs, kl_losses, 'r--', label='KL Loss', marker='s')
    plt.plot(epochs, task_losses, 'g--', label='Task Loss', marker='^')
    plt.plot(epochs, cosine_losses, 'm--', label='Cosine Loss', marker='d')
    plt.title('Training Loss Progress')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    
    # 2. ì •í™•ë„ ë¹„êµ
    plt.subplot(1, 3, 2)
    models = ['Teacher\n(KoKeyBERT)', 'Student\n(DistillKoKeyBERT)']
    accuracies = [eval_results['teacher']['accuracy'], eval_results['student']['accuracy']]
    f1_scores = [eval_results['teacher']['f1_score'], eval_results['student']['f1_score']]
    
    x = np.arange(len(models))
    width = 0.35
    
    plt.bar(x - width/2, accuracies, width, label='Accuracy', alpha=0.7, color='skyblue')
    plt.bar(x + width/2, f1_scores, width, label='F1 Score', alpha=0.7, color='lightcoral')
    
    plt.title('Model Performance Comparison')
    plt.xlabel('Model')
    plt.ylabel('Score')
    plt.xticks(x, models)
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # 3. ì¶”ë¡  ì†ë„ ë¹„êµ
    plt.subplot(1, 3, 3)
    times = [eval_results['teacher']['avg_time_ms'], eval_results['student']['avg_time_ms']]
    colors = ['red', 'blue']
    
    plt.bar(models, times, color=colors, alpha=0.7)
    plt.title('Inference Speed Comparison')
    plt.xlabel('Model')
    plt.ylabel('Time (ms)')
    plt.grid(True, alpha=0.3)
    
    # ì†ë„ í–¥ìƒ í‘œì‹œ
    plt.text(1, times[1] + max(times) * 0.1, f'{eval_results["speedup"]:.2f}x faster', 
             ha='center', fontweight='bold', color='blue')
    
    plt.tight_layout()
    plt.savefig('../results/plots/real_distillation_results.png', dpi=300, bbox_inches='tight')
    print("âœ… ê²°ê³¼ ì‹œê°í™” ì €ì¥ ì™„ë£Œ")
    
    # ìš”ì•½ ì¶œë ¥
    teacher_params = sum(p.numel() for p in teacher_model.parameters())
    student_params = sum(p.numel() for p in student_model.parameters())
    compression_ratio = student_params / teacher_params
    
    print("\n" + "="*70)
    print("ğŸ“Š ì‹¤ì œ Knowledge Distillation ì‹¤í—˜ ê²°ê³¼ ìš”ì•½")
    print("="*70)
    print(f"ğŸ« Teacher (KoKeyBERT):")
    print(f"   - íŒŒë¼ë¯¸í„°: {teacher_params:,}")
    print(f"   - ì •í™•ë„: {eval_results['teacher']['accuracy']:.4f}")
    print(f"   - ì •ë°€ë„: {eval_results['teacher']['precision']:.4f}")
    print(f"   - ì¬í˜„ìœ¨: {eval_results['teacher']['recall']:.4f}")
    print(f"   - F1 ì ìˆ˜: {eval_results['teacher']['f1_score']:.4f}")
    print(f"   - TP/FP/FN: {eval_results['teacher']['tp']}/{eval_results['teacher']['fp']}/{eval_results['teacher']['fn']}")
    print(f"   - ì¶”ë¡  ì‹œê°„: {eval_results['teacher']['avg_time_ms']:.2f}ms")
    print()
    print(f"ğŸ“ Student (DistillKoKeyBERT):")
    print(f"   - íŒŒë¼ë¯¸í„°: {student_params:,}")
    print(f"   - ì •í™•ë„: {eval_results['student']['accuracy']:.4f}")
    print(f"   - ì •ë°€ë„: {eval_results['student']['precision']:.4f}")
    print(f"   - ì¬í˜„ìœ¨: {eval_results['student']['recall']:.4f}")
    print(f"   - F1 ì ìˆ˜: {eval_results['student']['f1_score']:.4f}")
    print(f"   - TP/FP/FN: {eval_results['student']['tp']}/{eval_results['student']['fp']}/{eval_results['student']['fn']}")
    print(f"   - ì¶”ë¡  ì‹œê°„: {eval_results['student']['avg_time_ms']:.2f}ms")
    print()
    print(f"ğŸ“ˆ ì„±ëŠ¥ ë¹„êµ:")
    print(f"   - ì••ì¶• ë¹„ìœ¨: {compression_ratio:.3f} ({(1-compression_ratio)*100:.1f}% ì••ì¶•)")
    print(f"   - ì†ë„ í–¥ìƒ: {eval_results['speedup']:.2f}x")
    print(f"   - ì •í™•ë„ ì°¨ì´: {eval_results['student']['accuracy'] - eval_results['teacher']['accuracy']:.4f}")
    print(f"   - F1 ì ìˆ˜ ì°¨ì´: {eval_results['student']['f1_score'] - eval_results['teacher']['f1_score']:.4f}")
    print(f"   - ìµœì¢… í›ˆë ¨ Loss: {training_history[-1]['avg_loss']:.4f}")
    print(f"   - Best Val Acc: {best_val_acc:.4f}")
    print("="*70)

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¯ ì‹¤ì œ KoKeyBERT Knowledge Distillation ì‹¤í—˜ ì‹œì‘!")
    print("=" * 80)
    
    # GPU ì„¤ì •
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    if torch.cuda.is_available():
        print(f"ğŸš€ GPU: {torch.cuda.get_device_name()}")
        print(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
    
    try:
        # 1. test.pyì™€ ë™ì¼í•œ ë°ì´í„° ë¡œë“œ
        print("ğŸ“Š test.py ë°©ì‹ìœ¼ë¡œ ë°ì´í„° ë¡œë”© ì¤‘...")
        
        # í›ˆë ¨ ë°ì´í„° ê²½ë¡œ
        train_data_path = "../../src/data/train_clean.json"
        test_data_path = "../../src/data/test_clean.json"
        
        # ê²½ë¡œ í™•ì¸ ë° ëŒ€ì•ˆ ê²½ë¡œ ì‹œë„
        def find_data_file(filename):
            possible_paths = [
                f"../../src/data/{filename}",
                f"../../../src/data/{filename}",
                f"../../data/{filename}",
                f"../data/{filename}",
                f"./{filename}"
            ]
            
            for path in possible_paths:
                if os.path.exists(path):
                    return path
            return None
        
        train_data_path = find_data_file("train_clean.json")
        test_data_path = find_data_file("test_clean.json")
        
        if not train_data_path:
            raise FileNotFoundError("train_clean.jsonì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        if not test_data_path:
            raise FileNotFoundError("test_clean.jsonì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        print(f"âœ… í›ˆë ¨ ë°ì´í„° ê²½ë¡œ: {train_data_path}")
        print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ: {test_data_path}")
        
        # test.py ë°©ì‹ìœ¼ë¡œ ë°ì´í„° ë¡œë“œ
        train_data = load_data(train_data_path)
        test_data = load_data(test_data_path)
        
        if train_data is None or len(train_data) == 0:
            raise ValueError(f"í›ˆë ¨ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {train_data_path}")
        if test_data is None or len(test_data) == 0:
            raise ValueError(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {test_data_path}")
        
        print(f"âœ… í›ˆë ¨ ë°ì´í„° ë¡œë“œ: {len(train_data)} í•­ëª©")
        print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ: {len(test_data)} í•­ëª©")
        
        # 2. í† í¬ë‚˜ì´ì € ë¡œë“œ
        print("ğŸ”¤ í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
        try:
            # kobert_tokenizer í´ë”ì—ì„œ KoBERTTokenizer ê°€ì ¸ì˜¤ê¸°
            import sys
            sys.path.append('../../kobert_tokenizer')
            from kobert_tokenizer import KoBERTTokenizer
            tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')
            print("âœ… KoBERT (KoBERTTokenizer) ë¡œë“œ ì„±ê³µ")
        except Exception as e:
            print(f"âš ï¸ KoBERT í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ğŸ”„ ê¸°ë³¸ BERT í† í¬ë‚˜ì´ì €ë¡œ ëŒ€ì²´...")
            from transformers import BertTokenizer
            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
            print("âœ… ê¸°ë³¸ BERT í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ")
        
        # 3. test.py ë°©ì‹ìœ¼ë¡œ Datasetê³¼ DataLoader ìƒì„±
        print("ğŸ“¦ test.py ë°©ì‹ìœ¼ë¡œ Dataset ìƒì„± ì¤‘...")
        train_dataset = KeywordDataset(train_data)
        test_dataset = KeywordDataset(test_data)
        
        # Collator ìƒì„±
        collator = Collator(tokenizer)
        
        # test.py ë°©ì‹ìœ¼ë¡œ DataLoader ìƒì„±
        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0, collate_fn=collator, pin_memory=False)
        test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0, collate_fn=collator, pin_memory=False)
        
        # 4. ì‹¤ì œ ëª¨ë¸ë“¤ ìƒì„±
        teacher_model, student_model = create_real_models(device)
        
        # 5. Knowledge Distillation í›ˆë ¨
        training_history, best_model_state, best_val_acc = train_real_distillation(
            teacher_model, student_model, train_loader, test_loader, device, tokenizer, num_epochs=5
        )
        
        # 6. ëª¨ë¸ í‰ê°€
        eval_results = evaluate_real_models(teacher_model, student_model, test_loader, device, tokenizer)
        
        # 7. ê²°ê³¼ ì €ì¥
        save_real_results(teacher_model, student_model, training_history, eval_results, device, best_val_acc)
        
        print("\nğŸ‰ ì‹¤ì œ Knowledge Distillation ì‹¤í—˜ ì™„ë£Œ!")
        print("ğŸ“ ê²°ê³¼ íŒŒì¼:")
        print("  - ../results/models/distilled_koKeyBERT.pt")
        print("  - ../results/plots/distillation_results.png")
        
    except Exception as e:
        print(f"âŒ ì‹¤í—˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

if __name__ == "__main__":
    main()
