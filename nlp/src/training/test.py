from ..data.dataset import KeywordDataset, Collator, load_data
from ..models.kokeybert import KoKeyBERT
from ..tokenizer.kobert_tokenizer import KoBERTTokenizer
from transformers import BertConfig
from sklearn.metrics import confusion_matrix, precision_recall_fscore_support
import numpy as np
from ..experiments.distillation.distill_model import DistillKoKeyBERT


import argparse
import logging
from torch.utils.data import DataLoader, SequentialSampler
import torch
import os
from torch.nn.utils.rnn import pad_sequence
import json
from datetime import datetime
from typing import List
# ì „ì—­ ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

def extract_keywords_from_bio_tags(tokens, bio_tags, attention_mask, tokenizer, device) -> List[str]:
    """
    BIO íƒœê·¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        tokens: í† í° ID í…ì„œ
        bio_tags: BIO íƒœê·¸ í…ì„œ (0: B, 1: I, 2: O)
        attention_mask: íŒ¨ë”© ë§ˆìŠ¤í¬ í…ì„œ
        tokenizer: í† í¬ë‚˜ì´ì €
        device: í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤
    
    Returns:
        list: ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    """
    keywords = []
    current_keyword = []
    previous_tag = None
    
    # í† í°ê³¼ íƒœê·¸ì˜ ê¸¸ì´ê°€ ë‹¤ë¥´ë©´ ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ
    if len(tokens) != len(bio_tags) or len(tokens) != len(attention_mask):
        logger.warning("í† í°, íƒœê·¸, ë§ˆìŠ¤í¬ì˜ ê¸¸ì´ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return []
    
    # í…ì„œë¥¼ CPUì—ì„œ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ì¤€ë¹„
    tokens_cpu = tokens.cpu() if torch.is_tensor(tokens) else tokens
    bio_tags_cpu = bio_tags.cpu() if torch.is_tensor(bio_tags) else bio_tags
    attention_mask_cpu = attention_mask.cpu() if torch.is_tensor(attention_mask) else attention_mask
    
    for i, (token, tag, mask) in enumerate(zip(tokens_cpu, bio_tags_cpu, attention_mask_cpu)):
        if not mask:  # íŒ¨ë”©ëœ í† í°ì€ ê±´ë„ˆë›°ê¸°
            continue
            
        # ì •ìˆ˜ ê°’ìœ¼ë¡œ ë³€í™˜
        token_val = token.item() if torch.is_tensor(token) else token
        tag_val = tag.item() if torch.is_tensor(tag) else tag
        
        if tag_val == 0:  # B íƒœê·¸
            # ì´ì „ í‚¤ì›Œë“œê°€ ìˆì—ˆë‹¤ë©´ ì €ì¥
            if current_keyword:
                try:
                    keyword = tokenizer.decode(current_keyword).strip()
                    if keyword and keyword not in keywords:
                        keywords.append(keyword)
                except Exception as e:
                    logger.warning(f"í† í° ë””ì½”ë”© ì˜¤ë¥˜: {e}")
            current_keyword = [token_val]
            previous_tag = tag_val
        elif tag_val == 1:  # I íƒœê·¸
            # B íƒœê·¸(0) ë‹¤ìŒì— ì˜¤ëŠ” I íƒœê·¸ë§Œ ì²˜ë¦¬ (O íƒœê·¸(2) ë‹¤ìŒì˜ I íƒœê·¸ëŠ” ë¬´ì‹œ)
            if previous_tag == 0 or previous_tag == 1:
                current_keyword.append(token_val)
            previous_tag = tag_val
        else:  # O íƒœê·¸(2)
            # ì´ì „ í‚¤ì›Œë“œê°€ ìˆì—ˆë‹¤ë©´ ì €ì¥
            if current_keyword:
                try:
                    keyword = tokenizer.decode(current_keyword).strip()
                    if keyword and keyword not in keywords:
                        keywords.append(keyword)
                except Exception as e:
                    logger.warning(f"í† í° ë””ì½”ë”© ì˜¤ë¥˜: {e}")
            current_keyword = []
            previous_tag = tag_val
    
    # ë§ˆì§€ë§‰ í‚¤ì›Œë“œ ì²˜ë¦¬
    if current_keyword:
        try:
            keyword = tokenizer.decode(current_keyword).strip()
            if keyword and keyword not in keywords:
                keywords.append(keyword)
        except Exception as e:
            logger.warning(f"í† í° ë””ì½”ë”© ì˜¤ë¥˜: {e}")
    
    return keywords

def evaluate_keywords(pred_keywords, true_keywords):
    """
    ì˜ˆì¸¡ëœ í‚¤ì›Œë“œì™€ ì‹¤ì œ í‚¤ì›Œë“œë¥¼ ë¹„êµí•˜ì—¬ confusion matrixë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Args:
        pred_keywords: ì˜ˆì¸¡ëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        true_keywords: ì‹¤ì œ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        tuple: (TP, FP, FN) ê°’
    """
    # í‚¤ì›Œë“œ ì •ê·œí™”: ê³µë°± ì²˜ë¦¬
    pred_set = {keyword.strip() for keyword in pred_keywords if keyword}
    true_set = {keyword.strip() for keyword in true_keywords if keyword}
    
    TP = len(pred_set.intersection(true_set))
    FP = len(pred_set - true_set)
    FN = len(true_set - pred_set)
    
    return TP, FP, FN

def test(model: KoKeyBERT,
         test_dataset: KeywordDataset,
         collator: Collator = None,
         args: argparse.Namespace = None,
         logger: logging.Logger = None,
         device: torch.device = None,
         ):

    if logger is None:
        logger = logging.getLogger(__name__)
    logger.info("KoKeyBERT í…ŒìŠ¤íŠ¸ ì‹œì‘")
    logger.info("ë°°ì¹˜ í¬ê¸°: %d", args.batch_size if hasattr(args, 'batch_size') else 1)
    start_time = datetime.now()
    logger.info("start time: %s", start_time.strftime("%Y-%m-%d %H:%M:%S"))
    sampler = SequentialSampler(test_dataset)
    test_dataloader = DataLoader(test_dataset, 
                                batch_size=args.batch_size, 
                                collate_fn=collator, 
                                sampler=sampler,
                                num_workers=args.num_workers,
                                drop_last=False)
    
    model.eval()
    total_test_loss = 0.0
    total_test_acc = 0.0
    test_step = 0
    
    # í‚¤ì›Œë“œ í‰ê°€ë¥¼ ìœ„í•œ í†µê³„
    total_TP = 0
    total_FP = 0
    total_FN = 0
    
    # ëª¨ë“  ì˜ˆì¸¡ ë° ì‹¤ì œ í‚¤ì›Œë“œ ì €ì¥ (ê²°ê³¼ ë¶„ì„ìš©)
    all_pred_keywords = []
    all_true_keywords = []
    with torch.no_grad():
        for batch in test_dataloader:
            test_step += 1
            
            # ë°°ì¹˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            try:
                index, text_ids, text_attention_mask, bio_tags = batch
            except ValueError as e:
                logger.error(f"ë°°ì¹˜ ë°ì´í„° íŒŒì‹± ì˜¤ë¥˜: {e}")
                continue

            # ëª¨ë¸ ì¶”ë¡ 
            try:
                log_likelihood, sequence_of_tags = model(input_ids=text_ids.to(device), 
                                                        attention_mask=text_attention_mask.to(device), 
                                                        tags=bio_tags.to(device), 
                                                        return_outputs=False)
                log_likelihood = -log_likelihood.mean()
                total_test_loss += log_likelihood.item()
            except Exception as e:
                logger.error(f"ëª¨ë¸ ì¶”ë¡  ì˜¤ë¥˜: {e}")
                continue
            
            # BIO íƒœê·¸ ì‹œí€€ìŠ¤ ì²˜ë¦¬
            tag_seqs = [torch.tensor(s, dtype=torch.long, device=device) for s in sequence_of_tags]
            padded = pad_sequence(tag_seqs, batch_first=True, padding_value=model.config.pad_token_id).to(device)
            
            # ì •í™•ë„ ê³„ì‚°
            mb_acc = (padded == bio_tags.to(device)).float()[text_attention_mask.to(device).bool()].mean()
            total_test_acc += mb_acc.item()
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ ë° í‰ê°€
            batch_TP = 0
            batch_FP = 0
            batch_FN = 0
            
            for i in range(len(text_ids)):
                try:
                    # ì˜ˆì¸¡ëœ í‚¤ì›Œë“œ ì¶”ì¶œ
                    pred_keywords = extract_keywords_from_bio_tags(
                        text_ids[i],
                        padded[i],
                        text_attention_mask[i],
                        collator.tokenizer,
                        device
                    )
                    
                    # ì‹¤ì œ í‚¤ì›Œë“œ ì¶”ì¶œ
                    try:
                        # index êµ¬ì¡°ì— ë”°ë¼ ì ì ˆíˆ ì ‘ê·¼
                        idx = index[i][0] if isinstance(index[i], (list, tuple)) else index[i]
                        if isinstance(test_dataset.data[idx], dict) and "keyword" in test_dataset.data[idx]:
                            true_keywords = test_dataset.data[idx]["keyword"]
                            # ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                            if not isinstance(true_keywords, list):
                                true_keywords = [true_keywords]
                        else:
                            logger.warning(f"ë°ì´í„°ì…‹ì— 'keyword' í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤: {idx}")
                            true_keywords = []
                    except (IndexError, KeyError) as e:
                        logger.warning(f"ë°ì´í„°ì…‹ ì¸ë±ì‹± ì˜¤ë¥˜: {e}")
                        true_keywords = []
                    
                    # ëª¨ë“  ì˜ˆì¸¡ ë° ì‹¤ì œ í‚¤ì›Œë“œ ì €ì¥
                    all_pred_keywords.append(pred_keywords)
                    all_true_keywords.append(true_keywords)
                    
                    # í‚¤ì›Œë“œ í‰ê°€
                    TP, FP, FN = evaluate_keywords(pred_keywords, true_keywords)
                    batch_TP += TP
                    batch_FP += FP
                    batch_FN += FN
                except Exception as e:
                    logger.warning(f"í‚¤ì›Œë“œ ì¶”ì¶œ ë° í‰ê°€ ì˜¤ë¥˜: {e}")
                    continue
            
            total_TP += batch_TP
            total_FP += batch_FP
            total_FN += batch_FN
            
            # í˜„ì¬ ë°°ì¹˜ì˜ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
            precision = batch_TP / (batch_TP + batch_FP) if (batch_TP + batch_FP) > 0 else 0
            recall = batch_TP / (batch_TP + batch_FN) if (batch_TP + batch_FN) > 0 else 0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
            
            # ë¡œê¹… (ìì„¸í•œ ë°°ì¹˜ë³„ ë¡œê¹…ì€ ë””ë²„ê·¸ ë ˆë²¨ë¡œ)
            if hasattr(args, 'log_freq') and (test_step % args.log_freq == 0 or test_step == 1):
                logger.info("Step: %d/%d, Loss: %.4f, Acc: %.4f, P: %.4f, R: %.4f, F1: %.4f", 
                           test_step, len(test_dataloader), log_likelihood.item(), mb_acc.item(), 
                           precision, recall, f1)
            else:
                logger.debug("Step: %d/%d, Loss: %.4f, Acc: %.4f, P: %.4f, R: %.4f, F1: %.4f", 
                           test_step, len(test_dataloader), log_likelihood.item(), mb_acc.item(), 
                           precision, recall, f1)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
    if test_step == 0:
        logger.warning("í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ëª¨ë“  ë°°ì¹˜ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        return 0.0, 0.0, 0.0, 0.0, 0.0
    
    # í‰ê·  ì†ì‹¤ ë° ì •í™•ë„ ê³„ì‚°
    total_test_loss /= test_step
    total_test_acc /= test_step
    
    # ì „ì²´ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
    total_precision = total_TP / (total_TP + total_FP) if (total_TP + total_FP) > 0 else 0
    total_recall = total_TP / (total_TP + total_FN) if (total_TP + total_FN) > 0 else 0
    total_f1 = 2 * total_precision * total_recall / (total_precision + total_recall) if (total_precision + total_recall) > 0 else 0
    
    logger.info("í…ŒìŠ¤íŠ¸ ì™„ë£Œ - ì´ %d ë°°ì¹˜", test_step)
    logger.info("ìµœì¢… ì†ì‹¤: %.4f, ì •í™•ë„: %.4f", total_test_loss, total_test_acc)
    logger.info("í‚¤ì›Œë“œ ì„±ëŠ¥ - Precision: %.4f, Recall: %.4f, F1: %.4f", total_precision, total_recall, total_f1)
    logger.info("Confusion Matrix - TP: %d, FP: %d, FN: %d", total_TP, total_FP, total_FN)
    logger.info("end time: %s", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    logger.info("total time: %s", datetime.now() - start_time)

    # ìƒì„¸ ë¶„ì„ì„ ìœ„í•œ ê²°ê³¼ ì €ì¥ (ì˜µì…˜)
    if hasattr(args, 'save_results') and args.save_results:
        try:
            # ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
            results = {
                'metrics': {
                    'loss': total_test_loss,
                    'accuracy': total_test_acc,
                    'precision': total_precision,
                    'recall': total_recall,
                    'f1': total_f1,
                    'TP': total_TP,
                    'FP': total_FP,
                    'FN': total_FN
                },
                'predictions': []
            }
            
            # ê° ìƒ˜í”Œì˜ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
            for i in range(len(all_pred_keywords)):
                results['predictions'].append({
                    'predicted': all_pred_keywords[i],
                    'true': all_true_keywords[i] if i < len(all_true_keywords) else []
                })
            
            with open(os.path.join('./results/json', f'test_results_{args.test_logger_name}.json'), 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ìƒì„¸ ê²°ê³¼ê°€ './results/json/test_results_{args.test_logger_name}.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.warning(f"ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    return total_test_loss, total_test_acc, total_precision, total_recall, total_f1

def test_with_keybert(test_dataset, args, logger, device=None):
    """
    KeyBERT ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    Args:
        test_dataset: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹
        args: íŒŒë¼ë¯¸í„° (model_name ë“±)
        logger: ë¡œê±°
        device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
    
    Returns:
        tuple: (ì •ë°€ë„, ì¬í˜„ìœ¨, F1 ì ìˆ˜)
    """
    logger.info("KeyBERT ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    logger.info("ë°°ì¹˜ í¬ê¸°: %d", args.batch_size if hasattr(args, 'batch_size') else 1)
    start_time = datetime.now()
    logger.info("start time: %s", start_time.strftime("%Y-%m-%d %H:%M:%S"))
    try:
        from keybert import KeyBERT
        from transformers import BertModel
        import torch
        
        # KeyBERT ëª¨ë¸ ì´ˆê¸°í™”
        model = BertModel.from_pretrained(args.model_name)
        model.to(device)
        kw_model = KeyBERT(model=model)
        logger.info(f"KeyBERT ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ: {args.model_name}")
        logger.info(f"device: {model.device}")
        
        # í‰ê°€ ì§€í‘œ ì´ˆê¸°í™”
        total_TP = 0
        total_FP = 0
        total_FN = 0
        
        # ëª¨ë“  ì˜ˆì¸¡ ë° ì‹¤ì œ í‚¤ì›Œë“œ ì €ì¥ (ê²°ê³¼ ë¶„ì„ìš©)
        all_pred_keywords = []
        all_true_keywords = []
        
        # ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ ìœ„í•´ GPU ìºì‹œ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì¤€ë¹„
        batch_size = args.batch_size if hasattr(args, 'batch_size') else 1
        num_samples = len(test_dataset.data)
        num_batches = (num_samples + batch_size - 1) // batch_size  # ì˜¬ë¦¼ ë‚˜ëˆ—ì…ˆ
        test_step = 0
        
        for batch_idx in range(0, num_samples, batch_size):
            test_step += 1
            batch_end = min(batch_idx + batch_size, num_samples)
            batch_data = test_dataset.data[batch_idx:batch_end]
            
            # ë°°ì¹˜ë³„ ì„±ëŠ¥ ì§€í‘œ
            batch_TP = 0
            batch_FP = 0
            batch_FN = 0
            
            for idx, data in enumerate(batch_data):
                # í…ìŠ¤íŠ¸ì™€ ì‹¤ì œ í‚¤ì›Œë“œ ì¶”ì¶œ
                if isinstance(data, dict) and "text" in data and "keyword" in data:
                    text = data["text"]
                    true_keywords = data["keyword"]
                    
                    # ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                    if not isinstance(true_keywords, list):
                        true_keywords = [true_keywords]
                    
                    # KeyBERTë¥¼ ì‚¬ìš©í•˜ì—¬ í‚¤ì›Œë“œ ì¶”ì¶œ
                    try:
                        keywords = kw_model.extract_keywords(
                            docs=text,
                            keyphrase_ngram_range=(1, 1),
                            top_n=args.num_keywords,
                        )
                        
                        # í‚¤ì›Œë“œë§Œ ì¶”ì¶œ (ì ìˆ˜ ì œì™¸)
                        pred_keywords = [kw[0] for kw in keywords]
                        
                        # ëª¨ë“  ì˜ˆì¸¡ ë° ì‹¤ì œ í‚¤ì›Œë“œ ì €ì¥
                        all_pred_keywords.append(pred_keywords)
                        all_true_keywords.append(true_keywords)
                        
                        # í‚¤ì›Œë“œ í‰ê°€
                        TP, FP, FN = evaluate_keywords(pred_keywords, true_keywords)
                        batch_TP += TP
                        batch_FP += FP
                        batch_FN += FN
                        
                    except Exception as e:
                        logger.warning(f"KeyBERT í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
                        import traceback
                        logger.debug(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
                        continue
                else:
                    logger.warning(f"ë°ì´í„° í˜•ì‹ ì˜¤ë¥˜: {batch_idx + idx}ë²ˆì§¸ ë°ì´í„°")
            
            # ë°°ì¹˜ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
            precision = batch_TP / (batch_TP + batch_FP) if (batch_TP + batch_FP) > 0 else 0
            recall = batch_TP / (batch_TP + batch_FN) if (batch_TP + batch_FN) > 0 else 0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
            
            # ë¡œê¹… (ìì„¸í•œ ë°°ì¹˜ë³„ ë¡œê¹…ì€ ë””ë²„ê·¸ ë ˆë²¨ë¡œ)
            if hasattr(args, 'log_freq') and (test_step % args.log_freq == 0 or test_step == 1):
                logger.info("Step: %d/%d, P: %.4f, R: %.4f, F1: %.4f", 
                           test_step, num_batches, precision, recall, f1)
            else:
                logger.debug("Step: %d/%d, P: %.4f, R: %.4f, F1: %.4f", 
                           test_step, num_batches, precision, recall, f1)
            
            # ì „ì²´ í†µê³„ì— ì¶”ê°€
            total_TP += batch_TP
            total_FP += batch_FP
            total_FN += batch_FN
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
        if test_step == 0:
            logger.warning("í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ëª¨ë“  ë°°ì¹˜ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            return 0.0, 0.0, 0.0
        
        # ì „ì²´ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        total_precision = total_TP / (total_TP + total_FP) if (total_TP + total_FP) > 0 else 0
        total_recall = total_TP / (total_TP + total_FN) if (total_TP + total_FN) > 0 else 0
        total_f1 = 2 * total_precision * total_recall / (total_precision + total_recall) if (total_precision + total_recall) > 0 else 0
        
        logger.info("KeyBERT í…ŒìŠ¤íŠ¸ ì™„ë£Œ - ì´ %d ë°°ì¹˜", test_step)
        logger.info("í‚¤ì›Œë“œ ì„±ëŠ¥ - Precision: %.4f, Recall: %.4f, F1: %.4f", total_precision, total_recall, total_f1)
        logger.info("Confusion Matrix - TP: %d, FP: %d, FN: %d", total_TP, total_FP, total_FN)
        logger.info("end time: %s", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
        logger.info("total time: %s", datetime.now() - start_time)
        # ìƒì„¸ ë¶„ì„ì„ ìœ„í•œ ê²°ê³¼ ì €ì¥ (ì˜µì…˜)
        if hasattr(args, 'save_results') and args.save_results:
            try:
                # ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
                results = {
                    'metrics': {
                        'precision': total_precision,
                        'recall': total_recall,
                        'f1': total_f1,
                        'TP': total_TP,
                        'FP': total_FP,
                        'FN': total_FN
                    },
                    'predictions': []
                }
                
                # ê° ìƒ˜í”Œì˜ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
                for i in range(len(all_pred_keywords)):
                    results['predictions'].append({
                        'predicted': all_pred_keywords[i],
                        'true': all_true_keywords[i] if i < len(all_true_keywords) else []
                    })
                
                with open(os.path.join('./results/json', f'keybert_results_{args.test_logger_name}.json'), 'w', encoding='utf-8') as f:
                    json.dump(results, f, ensure_ascii=False, indent=2)
                
                logger.info(f"ìƒì„¸ ê²°ê³¼ê°€ './results/json/keybert_results_{args.test_logger_name}.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                logger.warning(f"ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        return total_precision, total_recall, total_f1
        
    except ImportError:
        logger.error("KeyBERT ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤. 'pip install keybert'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return 0.0, 0.0, 0.0

def test_with_distill_kokeybert(test_dataset, args, logger, device=None):
    """
    DistillKoKeyBERT ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    Args:
        test_dataset: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹
        args: íŒŒë¼ë¯¸í„° (model_name, checkpoint_path ë“±)
        logger: ë¡œê±°
        device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
    
    Returns:
        tuple: (ì†ì‹¤, ì •í™•ë„, ì •ë°€ë„, ì¬í˜„ìœ¨, F1 ì ìˆ˜)
    """
    if DistillKoKeyBERT is None:
        logger.error("DistillKoKeyBERTë¥¼ importí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return 0.0, 0.0, 0.0, 0.0, 0.0
    
    try:
        from ...tokenizer.kobert_tokenizer import KoBERTTokenizer
    except ImportError:
        logger.error("KoBERTTokenizerë¥¼ importí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return 0.0, 0.0, 0.0, 0.0, 0.0
    
    logger.info("DistillKoKeyBERT í…ŒìŠ¤íŠ¸ ì‹œì‘")
    logger.info("ë°°ì¹˜ í¬ê¸°: %d", args.batch_size if hasattr(args, 'batch_size') else 1)
    start_time = datetime.now()
    logger.info("start time: %s", start_time.strftime("%Y-%m-%d %H:%M:%S"))
    
    try:
        # DistillKoKeyBERT ëª¨ë¸ ì´ˆê¸°í™”
        if hasattr(args, 'distill_checkpoint_path') and args.distill_checkpoint_path:
            try:
                checkpoint = torch.load(args.distill_checkpoint_path, map_location=device)
                
                # ğŸ’¡ [ìˆ˜ì •] Teacher ì„¤ì •ì„ ë¶ˆëŸ¬ì˜¨ í›„ ìˆ˜ì •í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë³€ê²½
                if isinstance(checkpoint, dict) and 'model_config' in checkpoint:
                    logger.info("ì €ì¥ëœ ëª¨ë¸ ì„¤ì • ì‚¬ìš© ì„±ê³µ!")
                    student_config = checkpoint['model_config']
                else:
                    logger.info("ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ì–´, ê¸°ë³¸ Student ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    student_config = BertConfig.from_pretrained(args.model_name)
                    student_config.num_hidden_layers = student_config.num_hidden_layers // 2 # 6 ë ˆì´ì–´
                
                # logger.info(f"Student ëª¨ë¸ ì„¤ì •: {student_config.num_hidden_layers} ë ˆì´ì–´")
                # 1. ì œì™¸í•˜ê³  ì‹¶ì€ í‚¤ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
                keys_to_exclude = ['_output_attentions', 'transformers_version', 'model_type']

                # 2. ë”•ì…”ë„ˆë¦¬ ì»´í”„ë¦¬í—¨ì…˜ì„ ì‚¬ìš©í•´ íŠ¹ì • í‚¤ê°€ ì—†ëŠ” ìƒˆ ë”•ì…”ë„ˆë¦¬ë¥¼ ë§Œë“­ë‹ˆë‹¤.
                filtered_config = {
                    key: value
                    for key, value in student_config.to_dict().items()
                    if key not in keys_to_exclude
                }

                # 3. í•„í„°ë§ëœ ê²°ê³¼ë¥¼ ë¡œê¹…í•©ë‹ˆë‹¤.
                filtered_config = BertConfig(**filtered_config)
                # logger.info(f"filtered Student ëª¨ë¸ ì„¤ì •: {filtered_config}")
                logger.info(f"Student ëª¨ë¸ ì„¤ì •: {student_config.num_hidden_layers}")
                model = DistillKoKeyBERT(config=filtered_config, num_class=3)
                
                # ê°€ì¤‘ì¹˜ ë¡œë“œ
                if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                    model.load_state_dict(checkpoint['model_state_dict'])
                else:
                    model.load_state_dict(checkpoint)
                logger.info("ì²´í¬í¬ì¸íŠ¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ")

            except Exception as e:
                logger.error(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
                import traceback
                logger.debug(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
                return 0.0, 0.0, 0.0, 0.0, 0.0
        else:
            # ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ì„ ê²½ìš°
            logger.info("ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œê°€ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ Student ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.")
            # ğŸ’¡ [ìˆ˜ì •] Teacher ì„¤ì •ì„ ë¶ˆëŸ¬ì˜¨ í›„ ìˆ˜ì •í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë³€ê²½
            student_config = BertConfig.from_pretrained(args.model_name)
            student_config.num_hidden_layers = student_config.num_hidden_layers // 2
            logger.info(f"ê¸°ë³¸ Student ëª¨ë¸ ì„¤ì •: {student_config.num_hidden_layers} ë ˆì´ì–´")
            model = DistillKoKeyBERT(config=student_config, num_class=3)
        
        logger.info("ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        model.to(device)
        model.eval()
        
        # í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
        tokenizer = KoBERTTokenizer.from_pretrained(args.model_name)
        collator = Collator(tokenizer)
        
        # í‰ê°€ ì§€í‘œ ì´ˆê¸°í™”
        total_loss = 0.0
        total_acc = 0.0
        total_TP = 0
        total_FP = 0
        total_FN = 0
        
        # ëª¨ë“  ì˜ˆì¸¡ ë° ì‹¤ì œ í‚¤ì›Œë“œ ì €ì¥ (ê²°ê³¼ ë¶„ì„ìš©)
        all_pred_keywords = []
        all_true_keywords = []
        
        # ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ ìœ„í•´ GPU ìºì‹œ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # ë°ì´í„°ë¡œë” ìƒì„±
        batch_size = args.batch_size if hasattr(args, 'batch_size') else 1
        test_dataloader = DataLoader(
            test_dataset,
            batch_size=batch_size,
            sampler=SequentialSampler(test_dataset),
            collate_fn=collator,
            num_workers=0
        )
        
        test_step = 0
        
        for batch in test_dataloader:
            test_step += 1
            
            # ë°°ì¹˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            try:
                index, text_ids, text_attention_mask, bio_tags = batch
            except ValueError as e:
                logger.error(f"ë°°ì¹˜ ë°ì´í„° íŒŒì‹± ì˜¤ë¥˜: {e}")
                continue

            # ëª¨ë¸ ì¶”ë¡  (ë°°ì¹˜ ë‹¨ìœ„ë¡œ í•œ ë²ˆë§Œ í˜¸ì¶œ)
            try:
                with torch.no_grad():
                    predicted_tags = model(input_ids=text_ids.to(device), 
                                        attention_mask=text_attention_mask.to(device))
                
                # BIO íƒœê·¸ ì‹œí€€ìŠ¤ ì²˜ë¦¬
                if isinstance(predicted_tags, list):
                    tag_seqs = [torch.tensor(s, dtype=torch.long, device=device) for s in predicted_tags]
                    padded = pad_sequence(tag_seqs, batch_first=True, padding_value=model.config.pad_token_id).to(device)
                else:
                    padded = predicted_tags.to(device)
                
                # ì •í™•ë„ ê³„ì‚°
                batch_acc = (padded == bio_tags.to(device)).float()[text_attention_mask.to(device).bool()].mean()
                total_acc += batch_acc.item()

            except Exception as e:
                logger.error(f"ëª¨ë¸ ì¶”ë¡  ì˜¤ë¥˜: {e}")
                continue
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ ë° í‰ê°€
            batch_TP = 0
            batch_FP = 0
            batch_FN = 0
            
            for i in range(len(text_ids)):
                try:
                    # ì˜ˆì¸¡ëœ í‚¤ì›Œë“œ ì¶”ì¶œ
                    pred_keywords = extract_keywords_from_bio_tags(
                        text_ids[i],
                        padded[i],
                        text_attention_mask[i],
                        tokenizer,
                        device
                    )
                    
                    # ì‹¤ì œ í‚¤ì›Œë“œ ì¶”ì¶œ
                    try:
                        # index êµ¬ì¡°ì— ë”°ë¼ ì ì ˆíˆ ì ‘ê·¼
                        idx = index[i][0] if isinstance(index[i], (list, tuple)) else index[i]
                        if isinstance(test_dataset.data[idx], dict) and "keyword" in test_dataset.data[idx]:
                            true_keywords = test_dataset.data[idx]["keyword"]
                            # ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                            if not isinstance(true_keywords, list):
                                true_keywords = [true_keywords]
                        else:
                            logger.warning(f"ë°ì´í„°ì…‹ì— 'keyword' í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤: {idx}")
                            true_keywords = []
                    except (IndexError, KeyError) as e:
                        logger.warning(f"ë°ì´í„°ì…‹ ì¸ë±ì‹± ì˜¤ë¥˜: {e}")
                        true_keywords = []
                    
                    # ëª¨ë“  ì˜ˆì¸¡ ë° ì‹¤ì œ í‚¤ì›Œë“œ ì €ì¥
                    all_pred_keywords.append(pred_keywords)
                    all_true_keywords.append(true_keywords)
                    
                    # í‚¤ì›Œë“œ í‰ê°€
                    TP, FP, FN = evaluate_keywords(pred_keywords, true_keywords)
                    batch_TP += TP
                    batch_FP += FP
                    batch_FN += FN
                except Exception as e:
                    logger.warning(f"í‚¤ì›Œë“œ ì¶”ì¶œ ë° í‰ê°€ ì˜¤ë¥˜: {e}")
                    continue
            
            total_TP += batch_TP
            total_FP += batch_FP
            total_FN += batch_FN
            
            # í˜„ì¬ ë°°ì¹˜ì˜ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
            precision = batch_TP / (batch_TP + batch_FP) if (batch_TP + batch_FP) > 0 else 0
            recall = batch_TP / (batch_TP + batch_FN) if (batch_TP + batch_FN) > 0 else 0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
            
            # ë¡œê¹… (ìì„¸í•œ ë°°ì¹˜ë³„ ë¡œê¹…ì€ ë””ë²„ê·¸ ë ˆë²¨ë¡œ)
            if hasattr(args, 'log_freq') and (test_step % args.log_freq == 0 or test_step == 1):
                logger.info("Step: %d/%d, Acc: %.4f, P: %.4f, R: %.4f, F1: %.4f", 
                           test_step, len(test_dataloader), batch_acc.item(), 
                           precision, recall, f1)
            else:
                logger.debug("Step: %d/%d, Acc: %.4f, P: %.4f, R: %.4f, F1: %.4f", 
                           test_step, len(test_dataloader), batch_acc.item(), 
                           precision, recall, f1)
            
            # ì „ì²´ í†µê³„ì— ì¶”ê°€
            # total_acc += batch_acc.item()
            total_TP += batch_TP
            total_FP += batch_FP
            total_FN += batch_FN
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
        if test_step == 0:
            logger.warning("í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ëª¨ë“  ë°°ì¹˜ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            return 0.0, 0.0, 0.0, 0.0, 0.0
        
        # ì „ì²´ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        total_acc /= test_step
        total_precision = total_TP / (total_TP + total_FP) if (total_TP + total_FP) > 0 else 0
        total_recall = total_TP / (total_TP + total_FN) if (total_TP + total_FN) > 0 else 0
        total_f1 = 2 * total_precision * total_recall / (total_precision + total_recall) if (total_precision + total_recall) > 0 else 0
        
        logger.info("DistillKoKeyBERT í…ŒìŠ¤íŠ¸ ì™„ë£Œ - ì´ %d ë°°ì¹˜", test_step)
        logger.info("í‚¤ì›Œë“œ ì„±ëŠ¥ - Precision: %.4f, Recall: %.4f, F1: %.4f", total_precision, total_recall, total_f1)
        logger.info("Confusion Matrix - TP: %d, FP: %d, FN: %d", total_TP, total_FP, total_FN)
        logger.info("end time: %s", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
        logger.info("total time: %s", datetime.now() - start_time)
        
        # ìƒì„¸ ë¶„ì„ì„ ìœ„í•œ ê²°ê³¼ ì €ì¥ (ì˜µì…˜)
        if hasattr(args, 'save_results') and args.save_results:
            try:
                # ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
                results = {
                    'metrics': {
                        'accuracy': total_acc,
                        'precision': total_precision,
                        'recall': total_recall,
                        'f1': total_f1,
                        'TP': total_TP,
                        'FP': total_FP,
                        'FN': total_FN
                    },
                    'predictions': []
                }
                
                # ê° ìƒ˜í”Œì˜ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
                for i in range(len(all_pred_keywords)):
                    results['predictions'].append({
                        'predicted': all_pred_keywords[i],
                        'true': all_true_keywords[i] if i < len(all_true_keywords) else []
                    })
                
                with open(os.path.join('./results/json', f'distill_kokeybert_results_{args.test_logger_name}.json'), 'w', encoding='utf-8') as f:
                    json.dump(results, f, ensure_ascii=False, indent=2)
                
                logger.info(f"ìƒì„¸ ê²°ê³¼ê°€ './results/json/distill_kokeybert_results_{args.test_logger_name}.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                logger.warning(f"ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        return 0.0, total_acc, total_precision, total_recall, total_f1
        
    except Exception as e:
        logger.error(f"DistillKoKeyBERT í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        return 0.0, 0.0, 0.0, 0.0, 0.0

def main():
    parser = argparse.ArgumentParser(description='KoKeyBERT Testing')
    parser.add_argument("--test_data_path", type=str, default="../../../src/data/test_clean.json")
    parser.add_argument("--model_name", type=str, default="skt/kobert-base-v1")
    parser.add_argument("--batch_size", type=int, default=8)
    parser.add_argument("--device", type=str, default="cuda" if torch.cuda.is_available() else "cpu", help="cuda for gpu, cpu for cpu")
    parser.add_argument("--test_logger_name", type=str, default="test")
    parser.add_argument("--num_workers", type=int, default=8, help="A100: 12, 8 recommanded")
    parser.add_argument("--checkpoint_path", type=str, default="../../models/best_model.pt", help="Path to checkpoint to load model from")
    parser.add_argument("--log_freq", type=int, default=5, help="ë¡œê¹… ë¹ˆë„ (ëª‡ ë°°ì¹˜ë§ˆë‹¤ ë¡œê·¸ë¥¼ ì¶œë ¥í• ì§€)")
    parser.add_argument("--save_results", action="store_true", help="í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥")
    parser.add_argument("--use_keybert", action="store_true", help="KeyBERT ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸")
    parser.add_argument("--use_distill_kokeybert", action="store_true", help="DistillKoKeyBERT ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸")
    parser.add_argument("--distill_checkpoint_path", type=str, default="", help="DistillKoKeyBERT ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ")
    parser.add_argument("--num_keywords", type=int, default=3, help="í‚¤ì›Œë“œ ê°œìˆ˜")
    args = parser.parse_args()

    # Create necessary directories
    os.makedirs("./log", exist_ok=True)
    os.makedirs("./results", exist_ok=True)

    # Initialize logger
    logger = logging.getLogger(args.test_logger_name)
    logger.setLevel(logging.DEBUG)
    
    # ì´ë¯¸ í•¸ë“¤ëŸ¬ê°€ ìˆëŠ” ê²½ìš° ì œê±°
    if logger.handlers:
        logger.handlers.clear()

    # stream handler
    stream_handler = logging.StreamHandler()
    stream_handler.setLevel(logging.INFO)

    # file handler
    file_handler = logging.FileHandler("./log/" + args.test_logger_name + ".log")
    file_handler.setLevel(logging.DEBUG)

    # format
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    stream_handler.setFormatter(formatter)
    file_handler.setFormatter(formatter)

    logger.addHandler(stream_handler)
    logger.addHandler(file_handler)

    # Set device
    try:
        if args.device == "cuda" and torch.cuda.is_available():
            device = torch.device("cuda")
            logger.info(f"GPU ì‚¬ìš©: {torch.cuda.get_device_name(0)}")
        else:
            device = torch.device("cpu")
            logger.info("CPU ì‚¬ìš©")
    except Exception as e:
        logger.warning(f"ë””ë°”ì´ìŠ¤ ì„¤ì • ì˜¤ë¥˜: {e}, CPU ì‚¬ìš©")
        device = torch.device("cpu")

    # Load test data
    try:
        test_data = load_data(args.test_data_path)
        if test_data is None or len(test_data) == 0:
            raise ValueError(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ ë˜ëŠ” ë¹„ì–´ìˆìŒ: {args.test_data_path}")
        logger.info(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(test_data)} í•­ëª©")
        test_dataset = KeywordDataset(test_data)
    except Exception as e:
        logger.error(f"ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
        return

    # KeyBERT ë¼ì´ë¸ŒëŸ¬ë¦¬ í…ŒìŠ¤íŠ¸
    if args.use_keybert:
        logger.info("********** KeyBERT ë¼ì´ë¸ŒëŸ¬ë¦¬ í…ŒìŠ¤íŠ¸ ì‹œì‘ **********")
        keybert_precision, keybert_recall, keybert_f1 = test_with_keybert(
            test_dataset=test_dataset,
            args=args,
            logger=logger,
            device=device
        )
        logger.info("********** KeyBERT ë¼ì´ë¸ŒëŸ¬ë¦¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ **********")
        logger.info("KeyBERT ì„±ëŠ¥ - Precision: %.4f, Recall: %.4f, F1: %.4f", 
                   keybert_precision, keybert_recall, keybert_f1)
        return

    # DistillKoKeyBERT ëª¨ë¸ í…ŒìŠ¤íŠ¸
    elif args.use_distill_kokeybert:
        logger.info("********** DistillKoKeyBERT í…ŒìŠ¤íŠ¸ ì‹œì‘ **********")
        distill_loss, distill_acc, distill_precision, distill_recall, distill_f1 = test_with_distill_kokeybert(
            test_dataset=test_dataset,
            args=args,
            logger=logger,
            device=device
        )
        logger.info("********** DistillKoKeyBERT í…ŒìŠ¤íŠ¸ ì™„ë£Œ **********")
        logger.info("DistillKoKeyBERT ìµœì¢… ê²°ê³¼ - ì†ì‹¤: %.4f, ì •í™•ë„: %.4f", distill_loss, distill_acc)
        logger.info("DistillKoKeyBERT ì„±ëŠ¥ - Precision: %.4f, Recall: %.4f, F1: %.4f", 
                   distill_precision, distill_recall, distill_f1)
        return

    # KoKeyBERT ëª¨ë¸ í…ŒìŠ¤íŠ¸
    else:
        # Initialize tokenizer and collator
        try:
            tokenizer = KoBERTTokenizer.from_pretrained(args.model_name)
            collator = Collator(tokenizer)
            logger.info(f"í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            return

        # Load model from checkpoint
        try:
            logger.info(f"ëª¨ë¸ ë¡œë“œ ì¤‘: {args.checkpoint_path}")
            config = BertConfig.from_pretrained(args.model_name)
            model = KoKeyBERT(config=config)
            model.load_state_dict(torch.load(args.checkpoint_path, map_location=device))
            model.to(device)
            logger.info("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
            return

        # Run test
        logger.info("********** KoKeyBERT í…ŒìŠ¤íŠ¸ ì‹œì‘ **********")
        logger.info("ë°°ì¹˜ í¬ê¸°: %d", args.batch_size)
        
        try:
            kokeybert_loss, kokeybert_acc, kokeybert_precision, kokeybert_recall, kokeybert_f1 = test(
                model=model,
                test_dataset=test_dataset,
                collator=collator,
                args=args,
                logger=logger,
                device=device
            )
            
            logger.info("********** KoKeyBERT í…ŒìŠ¤íŠ¸ ì™„ë£Œ **********")
            logger.info("KoKeyBERT ìµœì¢… ê²°ê³¼ - ì†ì‹¤: %.4f, ì •í™•ë„: %.4f", kokeybert_loss, kokeybert_acc)
            logger.info("KoKeyBERT ì„±ëŠ¥ - Precision: %.4f, Recall: %.4f, F1: %.4f", 
                       kokeybert_precision, kokeybert_recall, kokeybert_f1)
        except Exception as e:
            logger.error(f"í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    main()
